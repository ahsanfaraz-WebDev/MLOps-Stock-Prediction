\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{amssymb}

\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\rhead{MLOps Stock Prediction Project}
\lhead{Complete Documentation}
\cfoot{\thepage}

% Define professional colors
\definecolor{primaryblue}{RGB}{0,51,102}
\definecolor{secondaryblue}{RGB}{0,102,204}
\definecolor{accentgold}{RGB}{255,204,0}
\definecolor{darkgray}{RGB}{64,64,64}

\title{
    \vspace{-1.5cm}
    \centering
    \textcolor{primaryblue}{\rule{\textwidth}{3pt}}\\[0.5cm]
    \textcolor{primaryblue}{\textbf{\Huge MLOps Project}}\\[0.8cm]
    \textcolor{secondaryblue}{\textbf{\Large MLOps Case Study}}\\[0.4cm]
    \textcolor{darkgray}{\textbf{\large Building a Real-Time Predictive System (RPS)}}\\[0.6cm]
    \textcolor{darkgray}{\large Complete Implementation Documentation}\\
    \textcolor{darkgray}{\large From Initialization to Production Deployment}\\[0.5cm]
    \textcolor{primaryblue}{\rule{\textwidth}{3pt}}\\[1cm]
}
\author{
    \centering
    \begin{tabular}{l}
        \textcolor{primaryblue}{\textbf{\large Group Members:}}\\[0.3cm]
        \textcolor{darkgray}{1. Ahsan Faraz \hspace{2cm} i228791} \\[0.2cm]
        \textcolor{darkgray}{2. Gulsher Khan \hspace{2cm} i222637} \\[0.2cm]
        \textcolor{darkgray}{3. Muhammad Faisal \hspace{1.5cm} i228758} \\[0.8cm]
        \textcolor{primaryblue}{\textbf{\large Submitted To:}}\\[0.3cm]
        \textcolor{darkgray}{Sir Pir Samiullah Shah}\\[0.8cm]
        \textcolor{primaryblue}{\textbf{\large Institution:}}\\[0.3cm]
        \textcolor{darkgray}{FAST National University of Computer}\\
        \textcolor{darkgray}{and Emerging Sciences}
    \end{tabular}
}
\date{\vspace{0.8cm}\centering\textcolor{darkgray}{\today}}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={MLOps Stock Prediction Project Documentation}
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\begin{document}

% Title Page
\begin{titlepage}
\centering
\maketitle
\thispagestyle{empty}
\vfill
\begin{center}
\textcolor{secondaryblue}{\rule{0.8\textwidth}{1pt}}\\[0.5cm]
\textcolor{darkgray}{\textit{This document contains the complete implementation documentation for the MLOps Stock Prediction Project, covering all phases from project initialization to production deployment with monitoring and observability.}}\\[0.5cm]
\textcolor{secondaryblue}{\rule{0.8\textwidth}{1pt}}\\[0.8cm]
\textcolor{primaryblue}{\textbf{\large Project Links:}}\\[0.4cm]
\textcolor{secondaryblue}{\href{https://github.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction.git}{\textbf{GitHub Repository}}}\\[0.2cm]
\textcolor{secondaryblue}{\href{https://dagshub.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction}{\textbf{DAGsHub Project}}}
\end{center}
\vfill
\end{titlepage}

% Table of Contents
\newpage
\tableofcontents
\newpage

\section{Project Overview}

This document provides a comprehensive guide to the MLOps Stock Prediction Project implementation. The project implements a complete MLOps pipeline for stock price prediction using Alpha Vantage API, Apache Airflow for orchestration, MLflow for experiment tracking, and DVC for data versioning.

\subsection{Project Objectives}
\begin{itemize}
    \item Build an end-to-end MLOps pipeline for stock price prediction
    \item Implement automated data extraction from Alpha Vantage API
    \item Set up data quality checks and transformations
    \item Train machine learning models with experiment tracking
    \item Version control data and models using DVC
    \item Orchestrate the entire pipeline using Apache Airflow
\end{itemize}

\subsection{Technology Stack}
\begin{itemize}
    \item \textbf{Orchestration}: Apache Airflow 2.7.0
    \item \textbf{Data Versioning}: DVC (Data Version Control) with AWS S3 remote storage
    \item \textbf{Cloud Storage}: AWS S3 (Amazon Simple Storage Service)
    \item \textbf{Experiment Tracking}: MLflow with Dagshub integration
    \item \textbf{Model Registry}: MLflow Model Registry
    \item \textbf{Containerization}: Docker and Docker Compose
    \item \textbf{CI/CD}: GitHub Actions
    \item \textbf{Model Comparison}: CML (Continuous Machine Learning)
    \item \textbf{API Framework}: FastAPI
    \item \textbf{Monitoring}: Prometheus and Grafana
    \item \textbf{Metrics}: prometheus-client
    \item \textbf{API}: Alpha Vantage Stock API
    \item \textbf{ML Framework}: scikit-learn (RandomForestRegressor)
    \item \textbf{Data Processing}: pandas, numpy
    \item \textbf{Data Quality}: ydata-profiling, great-expectations
    \item \textbf{Container Registry}: Docker Hub
\end{itemize}

\newpage

\section{Step 1: Project Initialization}

\subsection{Objective}
Set up the project structure, initialize Git repository, and configure basic project files.

\subsection{Commands Executed}
\begin{verbatim}
mkdir MLOps-Stock-Prediction
cd MLOps-Stock-Prediction
git init
git checkout -b dev
\end{verbatim}

\subsection{Files Created}
\begin{itemize}
    \item \texttt{.gitignore} - Git ignore patterns for Python, data, models, and Airflow files
    \item \texttt{README.md} - Project documentation (if created)
\end{itemize}

\subsection{File Purposes}
\begin{itemize}
    \item \texttt{.gitignore}: Prevents committing unnecessary files like \_\_pycache\_\_, .env, data files, model files, and Airflow logs
\end{itemize}

\subsection{Directory Structure Created}
\begin{verbatim}
MLOps-Stock-Prediction/
|-- .git/
|-- .gitignore
`-- README.md (optional)
\end{verbatim}

\newpage

\section{Step 2: Environment Configuration}

\subsection{Objective}
Set up environment variables and configuration files for API keys and service credentials.

\subsection{Files Created}
\begin{itemize}
    \item \texttt{.env} - Environment variables template (not committed to Git)
\end{itemize}

\subsection{File Contents Structure}
The \texttt{.env} file contains:
\begin{itemize}
    \item \texttt{ALPHA\_VANTAGE\_KEY} - API key for Alpha Vantage
    \item \texttt{DAGSHUB\_USERNAME} - Dagshub username
    \item \texttt{DAGSHUB\_TOKEN} - Dagshub authentication token
    \item \texttt{MLFLOW\_TRACKING\_URI} - MLflow tracking server URI
    \item \texttt{STOCK\_SYMBOL} - Stock symbol to predict (default: AAPL)
    \item \texttt{AIRFLOW\_UID} - Airflow user ID for Docker (default: 50000)
    \item \texttt{\_AIRFLOW\_WWW\_USER\_USERNAME} - Airflow webserver username
    \item \texttt{\_AIRFLOW\_WWW\_USER\_PASSWORD} - Airflow webserver password
\end{itemize}

\subsection{Purpose}
The \texttt{.env} file stores sensitive credentials and configuration that should not be committed to version control. It is loaded by Python scripts using the \texttt{python-dotenv} library.

\newpage

\section{Step 3: Source Code Structure Setup}

\subsection{Objective}
Create the source code directory structure and configuration module.

\subsection{Commands Executed}
\begin{verbatim}
mkdir src
mkdir data
mkdir data/raw
mkdir data/processed
mkdir models
mkdir airflow
mkdir airflow/dags
mkdir airflow/logs
mkdir airflow/plugins
mkdir docker
mkdir tests
mkdir monitoring
\end{verbatim}

\subsection{Files Created}
\begin{itemize}
    \item \texttt{src/\_\_init\_\_.py} - Python package marker for src directory
    \item \texttt{src/config.py} - Centralized configuration module
\end{itemize}

\subsection{File Purposes}
\begin{itemize}
    \item \texttt{src/\_\_init\_\_.py}: Makes \texttt{src} a Python package, enabling imports
    \item \texttt{src/config.py}: Centralizes all configuration settings including:
    \begin{itemize}
        \item API keys (Alpha Vantage)
        \item Dagshub credentials
        \item MLflow tracking URI
        \item Data directory paths
        \item Model configuration (random state, test size)
        \item Data quality thresholds
        \item Monitoring ports
    \end{itemize}
\end{itemize}

\subsection{Directory Structure}
\begin{verbatim}
MLOps-Stock-Prediction/
|-- src/
|   |-- __init__.py
|   `-- config.py
|-- data/
|   |-- raw/          (for raw data files)
|   `-- processed/    (for processed data files)
|-- models/           (for trained models)
|-- airflow/
|   |-- dags/         (for Airflow DAG files)
|   |-- logs/         (for Airflow logs)
|   `-- plugins/      (for Airflow plugins)
|-- docker/           (for Docker files)
|-- tests/            (for test files)
`-- monitoring/       (for monitoring configs)
\end{verbatim}

\newpage

\section{Step 4: Python Dependencies Setup}

\subsection{Objective}
Define all Python package dependencies required for the project.

\subsection{Files Created}
\begin{itemize}
    \item \texttt{requirements.txt} - Python package dependencies list
\end{itemize}

\subsection{File Contents}
The \texttt{requirements.txt} includes:
\begin{itemize}
    \item \textbf{Core ML Libraries}: pandas ($\geq$2.0.0, $<$2.2.0), numpy ($\geq$1.24.0, $<$1.27.0), scikit-learn ($\geq$1.3.0)
    \item \textbf{API and Web Framework}: requests ($\geq$2.31.0), fastapi ($\geq$0.100.0), uvicorn ($\geq$0.23.0), pydantic ($\geq$2.0.0, $<$2.6.0)
    \item \textbf{MLOps Tools}: mlflow ($\geq$2.8.0), dvc ($\geq$3.42.0), dvc[s3] ($\geq$3.42.0), boto3 ($\geq$1.28.0)
    \item \textbf{Monitoring}: prometheus-client ($\geq$0.17.0)
    \item \textbf{Data Quality}: ydata-profiling ($\geq$4.0.0), great-expectations ($\geq$0.18.0)
    \item \textbf{Utilities}: python-dotenv ($\geq$1.0.0), pytest ($\geq$7.4.0), flake8 ($\geq$6.0.0), joblib ($\geq$1.3.0)
\end{itemize}

\subsection{Important Notes}
\begin{itemize}
    \item Version constraints are set for Python 3.8 compatibility (Airflow 2.7.0 uses Python 3.8)
    \item \texttt{apache-airflow} is NOT included as it's part of the base Docker image
    \item \texttt{pandas-profiling} is replaced with \texttt{ydata-profiling} (deprecated)
\end{itemize}

\subsection{Issues Encountered and Solutions}

\subsubsection{Issue 1: Python Version Compatibility}
\textbf{Problem}: Initial requirements.txt had versions incompatible with Python 3.8 (used in Airflow Docker image).

\textbf{Solution}: Updated package versions to be compatible with Python 3.8:
\begin{itemize}
    \item pandas: Changed to $\geq$2.0.0, $<$2.2.0
    \item numpy: Changed to $\geq$1.24.0, $<$1.27.0
    \item scikit-learn: Changed to $\geq$1.3.0
\end{itemize}

\subsubsection{Issue 2: pandas-profiling Deprecation}
\textbf{Problem}: \texttt{pandas-profiling} package is deprecated.

\textbf{Solution}: Replaced with \texttt{ydata-profiling} ($\geq$4.0.0).

\newpage

\section{Step 5: Data Extraction Module}

\subsection{Objective}
Create a module to fetch stock data from Alpha Vantage API.

\subsection{Files Created}
\begin{itemize}
    \item \texttt{src/data\_extraction.py} - Stock data extraction script
\end{itemize}

\subsection{File Purpose}
The \texttt{src/data\_extraction.py} file:
\begin{itemize}
    \item Fetches intraday stock data from Alpha Vantage API
    \item Uses TIME\_SERIES\_INTRADAY function with 60-minute intervals
    \item Handles API rate limits and errors
    \item Saves data to CSV format in \texttt{data/raw/} directory
    \item Returns the file path for use in downstream tasks
    \item Includes timestamp in filename for versioning
\end{itemize}

\subsection{Key Functions}
\begin{itemize}
    \item \texttt{fetch\_stock\_data()} - Main function that fetches and saves stock data
\end{itemize}

\subsection{Generated Files}
When executed, this script generates:
\begin{itemize}
    \item \texttt{data/raw/stock\_data\_\{SYMBOL\}\_\{TIMESTAMP\}.csv} - Raw stock data CSV file
\end{itemize}

\subsection{Issues Encountered and Solutions}

\subsubsection{Issue 1: API Rate Limits}
\textbf{Problem}: Using \texttt{outputsize='full'} hit API rate limits on free tier.

\textbf{Solution}: Changed to \texttt{outputsize='compact'} which returns last 100 data points (free tier compatible).

\subsubsection{Issue 2: API Error Handling}
\textbf{Problem}: API sometimes returns 'Information' key instead of data.

\textbf{Solution}: Added error handling for 'Information' responses with clear error messages indicating rate limits or API key issues.

\newpage

\section{Step 6: Data Quality Check Module}

\subsection{Objective}
Create a module to validate data quality before processing.

\subsection{Files Created}
\begin{itemize}
    \item \texttt{src/data\_quality\_check.py} - Data quality validation script
\end{itemize}

\subsection{File Purpose}
The \texttt{src/data\_quality\_check.py} file performs:
\begin{itemize}
    \item Empty DataFrame check
    \item Required columns validation (open, high, low, close, volume)
    \item Null value threshold check (default: 1\%)
    \item Negative price validation
    \item Negative volume validation
    \item High price $\geq$ Low price validation
    \item Returns True if all checks pass, False otherwise
\end{itemize}

\subsection{Key Functions}
\begin{itemize}
    \item \texttt{check\_data\_quality(file\_path, null\_threshold)} - Validates data quality
\end{itemize}

\subsection{Usage}
Called by Airflow DAG after data extraction to ensure data quality before transformation.

\newpage

\section{Step 7: Data Transformation Module}

\subsection{Objective}
Create a module for data cleaning, feature engineering, and data profiling.

\subsection{Files Created}
\begin{itemize}
    \item \texttt{src/data\_transformation.py} - Data transformation and feature engineering script
\end{itemize}

\subsection{File Purpose}
The \texttt{src/data\_transformation.py} file:
\begin{itemize}
    \item Loads raw stock data
    \item Creates time-series features:
    \begin{itemize}
        \item Lag features (1h, 2h, 3h, 6h, 12h, 24h, 48h ago)
        \item Rolling statistics (mean, std, min, max for various windows)
        \item Price changes and percentages
        \item High-Low range calculations
        \item Simple Moving Averages (SMA)
        \item Volatility calculations
        \item Time-based features (hour, day, month, etc.)
        \item Target variable (next hour's closing price)
    \end{itemize}
    \item Generates data profiling report using ydata-profiling
    \item Saves processed data to CSV
    \item Returns paths to processed data and profiling report
\end{itemize}

\subsection{Key Functions}
\begin{itemize}
    \item \texttt{create\_features(df)} - Creates all feature engineering transformations
    \item \texttt{transform\_data(input\_path)} - Main transformation pipeline
\end{itemize}

\subsection{Generated Files}
When executed, this script generates:
\begin{itemize}
    \item \texttt{data/processed/stock\_data\_processed\_\{SYMBOL\}\_\{TIMESTAMP\}.csv} - Processed data with features
    \item \texttt{data/processed/data\_profile\_report\_\{TIMESTAMP\}.html} - Data profiling HTML report
\end{itemize}

\newpage

\section{Step 8: Docker Configuration}

\subsection{Objective}
Set up Docker containers for Apache Airflow and dependencies.

\subsection{Files Created}
\begin{itemize}
    \item \texttt{docker/Dockerfile} - Custom Docker image for Airflow with project dependencies
    \item \texttt{docker-compose.yml} - Docker Compose configuration for all services
    \item \texttt{.dockerignore} - Files to exclude from Docker build context
\end{itemize}

\subsection{File Purposes}

\subsubsection{docker/Dockerfile}
\begin{itemize}
    \item Base image: \texttt{apache/airflow:2.7.0}
    \item Installs system packages: git, curl, vim
    \item Copies and installs Python dependencies from \texttt{requirements.txt}
    \item Sets PYTHONPATH environment variable
    \item Uses airflow user for security
\end{itemize}

\subsubsection{docker-compose.yml}
Defines four services:
\begin{enumerate}
    \item \textbf{postgres}: PostgreSQL database for Airflow metadata
    \begin{itemize}
        \item Image: postgres:13
        \item Database: airflow
        \item User: airflow
        \item Health check configured
    \end{itemize}
    \item \textbf{airflow-init}: Initialization service
    \begin{itemize}
        \item Runs database migrations
        \item Creates default Airflow user
        \item Depends on postgres being healthy
    \end{itemize}
    \item \textbf{airflow-webserver}: Airflow web UI
    \begin{itemize}
        \item Port: 8080
        \item Depends on airflow-init completion
        \item Health check configured
    \end{itemize}
    \item \textbf{airflow-scheduler}: Airflow scheduler
    \begin{itemize}
        \item Executes DAGs
        \item Depends on airflow-init completion
        \item Health check configured
    \end{itemize}
\end{enumerate}

\subsubsection{.dockerignore}
Excludes unnecessary files from Docker build:
\begin{itemize}
    \item .git, .dvc, .vscode, .idea
    \item \_\_pycache\_\_, *.pyc, *.log, *.db
    \item data/raw, data/processed
    \item models/*.pkl, models/*.joblib
    \item .env, venv, env
\end{itemize}

\subsection{Issues Encountered and Solutions}

\subsubsection{Issue 1: Docker Build Timeout}
\textbf{Problem}: Network timeout while downloading large packages during Docker build.

\textbf{Solution}: Added timeout and retry parameters to pip install:
\begin{verbatim}
pip install --no-cache-dir --timeout=300 --retries=5 -r requirements.txt
\end{verbatim}

\subsubsection{Issue 2: Airflow Initialization Failure}
\textbf{Problem}: \texttt{airflow-init} service failed with "Too old Airflow version" error.

\textbf{Solution}: 
\begin{itemize}
    \item Changed \texttt{airflow-init} to use base \texttt{apache/airflow:2.7.0} image directly
    \item Removed custom build for init service
    \item Relied on Airflow's built-in entrypoint for initialization
\end{itemize}

\subsubsection{Issue 3: PostgreSQL Connection Error}
\textbf{Problem}: \texttt{airflow-init} couldn't connect to PostgreSQL (service not ready).

\textbf{Solution}: Added \texttt{depends\_on} with \texttt{condition: service\_healthy} to ensure PostgreSQL is ready before initialization.

\subsubsection{Issue 4: DAG Not Appearing in UI}
\textbf{Problem}: DAG file not discovered by Airflow scheduler after 7+ minutes.

\textbf{Solution}:
\begin{itemize}
    \item Added explicit volume mount: \texttt{./airflow/dags:/opt/airflow/dags}
    \item Set environment variable: \texttt{AIRFLOW\_\_CORE\_\_DAGS\_FOLDER: /opt/airflow/dags}
    \item Fixed project root path in DAG file to \texttt{/opt/airflow}
\end{itemize}

\subsection{Commands Executed}
\begin{verbatim}
docker-compose build --no-cache
docker-compose up airflow-init
docker-compose up -d
docker-compose logs airflow-scheduler
docker-compose exec airflow-scheduler airflow dags list
\end{verbatim}

\newpage

\section{Step 9: Airflow DAG Creation}

\subsection{Objective}
Create Apache Airflow DAG to orchestrate the entire ETL and ML pipeline.

\subsection{Files Created}
\begin{itemize}
    \item \texttt{airflow/dags/stock\_prediction\_dag.py} - Main Airflow DAG definition
\end{itemize}

\subsection{File Purpose}
The \texttt{airflow/dags/stock\_prediction\_dag.py} file defines:
\begin{itemize}
    \item DAG configuration (schedule, catchup, tags)
    \item Five tasks in sequence:
    \begin{enumerate}
        \item \textbf{extract\_data}: Extracts stock data from Alpha Vantage API
        \item \textbf{data\_quality\_check}: Validates data quality
        \item \textbf{transform\_data}: Transforms data and creates features
        \item \textbf{version\_with\_dvc}: Versions data using DVC
        \item \textbf{train\_model}: Trains ML model
    \end{enumerate}
    \item Task dependencies: extract $\rightarrow$ quality\_check $\rightarrow$ transform $\rightarrow$ dvc\_version $\rightarrow$ train\_model
    \item XCom communication between tasks
\end{itemize}

\subsection{DAG Configuration}
\begin{itemize}
    \item \textbf{DAG ID}: stock\_prediction\_pipeline
    \item \textbf{Schedule}: @daily (runs once per day at midnight)
    \item \textbf{Catchup}: False (doesn't backfill past dates)
    \item \textbf{Owner}: mlops\_team
    \item \textbf{Retries}: 1
    \item \textbf{Retry Delay}: 5 minutes
\end{itemize}

\subsection{Task Details}

\subsubsection{Task 1: extract\_data}
\begin{itemize}
    \item Type: PythonOperator
    \item Function: \texttt{extract\_task()}
    \item Calls: \texttt{src.data\_extraction.fetch\_stock\_data()}
    \item Returns: File path to raw CSV data
\end{itemize}

\subsubsection{Task 2: data\_quality\_check}
\begin{itemize}
    \item Type: PythonOperator
    \item Function: \texttt{quality\_check\_task(**context)}
    \item Calls: \texttt{src.data\_quality\_check.check\_data\_quality()}
    \item Retrieves: File path from extract\_data task via XCom
    \item Raises: ValueError if quality check fails
\end{itemize}

\subsubsection{Task 3: transform\_data}
\begin{itemize}
    \item Type: PythonOperator
    \item Function: \texttt{transform\_task(**context)}
    \item Calls: \texttt{src.data\_transformation.transform\_data()}
    \item Retrieves: File path from data\_quality\_check task via XCom
    \item Returns: Dictionary with processed\_data\_path and report\_path
\end{itemize}

\subsubsection{Task 4: version\_with\_dvc}
\begin{itemize}
    \item Type: PythonOperator
    \item Function: \texttt{dvc\_version\_task(**context)}
    \item Operations:
    \begin{itemize}
        \item Adds processed data to DVC: \texttt{dvc add \{processed\_data\_path\}}
        \item Stages DVC metadata files: \texttt{git add *.dvc .dvc/config}
        \item Commits to Git: \texttt{git commit -m "Update data version"}
        \item Pushes data to AWS S3: \texttt{dvc push --remote myremote}
        \item Falls back to Dagshub if S3 push fails
    \end{itemize}
    \item Retrieves: Paths from transform\_data task via XCom
    \item Remote Storage: AWS S3 (\texttt{s3://mlops-stock-prediction-data/dvc-storage})
    \item Includes retry logic and error handling for cloud storage operations
\end{itemize}

\subsubsection{Task 5: train\_model}
\begin{itemize}
    \item Type: PythonOperator
    \item Function: \texttt{train\_model\_task(**context)}
    \item Executes: \texttt{src/train.py} script via subprocess
    \item Retrieves: Processed data path from transform\_data task via XCom
    \item Returns: Data path
\end{itemize}

\subsection{Issues Encountered and Solutions}

\subsubsection{Issue 1: Import Errors}
\textbf{Problem}: DAG couldn't import custom modules from src directory.

\textbf{Solution}: Added project root to Python path:
\begin{verbatim}
project_root = Path('/opt/airflow')
sys.path.insert(0, str(project_root))
\end{verbatim}

\subsubsection{Issue 2: XCom Data Retrieval}
\textbf{Problem}: Tasks couldn't retrieve data from previous tasks.

\textbf{Solution}: Used Airflow's XCom pull:
\begin{verbatim}
data_path = ti.xcom_pull(task_ids='extract_data')
\end{verbatim}

\subsubsection{Issue 3: Schedule Interval Deprecation}
\textbf{Problem}: \texttt{schedule\_interval} parameter deprecated in Airflow 2.x.

\textbf{Solution}: Changed to \texttt{schedule='@daily'} instead of \texttt{schedule\_interval='@daily'}.

\newpage

\section{Step 10: Model Training Module}

\subsection{Objective}
Create a script to train machine learning models with MLflow experiment tracking.

\subsection{Files Created}
\begin{itemize}
    \item \texttt{src/train.py} - Model training script with MLflow integration
\end{itemize}

\subsection{File Purpose}
The \texttt{src/train.py} file:
\begin{itemize}
    \item Loads processed data with features
    \item Splits data into train/test sets (80/20, no shuffle for time series)
    \item Trains RandomForestRegressor model
    \item Calculates metrics: RMSE, MAE, R²
    \item Logs everything to MLflow:
    \begin{itemize}
        \item Model parameters (n\_estimators, max\_depth, etc.)
        \item Training and test metrics
        \item Model artifact
        \item Feature importance CSV
    \end{itemize}
    \item Saves model locally as pickle file
    \item Returns model file path
\end{itemize}

\subsection{Key Functions}
\begin{itemize}
    \item \texttt{train\_model(data\_path)} - Main training function
\end{itemize}

\subsection{Model Configuration}
\begin{itemize}
    \item \textbf{Algorithm}: RandomForestRegressor
    \item \textbf{n\_estimators}: 100
    \item \textbf{max\_depth}: 10
    \item \textbf{min\_samples\_split}: 2
    \item \textbf{min\_samples\_leaf}: 1
    \item \textbf{random\_state}: 42
    \item \textbf{test\_size}: 0.2 (20\%)
\end{itemize}

\subsection{Generated Files}
When executed, this script generates:
\begin{itemize}
    \item \texttt{models/stock\_model.pkl} - Trained model pickle file
    \item \texttt{models/feature\_importance.csv} - Feature importance rankings
\end{itemize}

\subsection{MLflow Integration}
\begin{itemize}
    \item Tracking URI: Configured from \texttt{src/config.py}
    \item Run name: \texttt{stock\_prediction\_\{TIMESTAMP\}}
    \item Logged parameters: All model hyperparameters and data statistics
    \item Logged metrics: train\_rmse, test\_rmse, train\_mae, test\_mae, train\_r2, test\_r2
    \item Logged artifacts: Model file, feature importance CSV
\end{itemize}

\subsection{Issues Encountered and Solutions}

\subsubsection{Issue 1: Missing Training Script}
\textbf{Problem}: DAG task failed because \texttt{src/train.py} didn't exist.

\textbf{Solution}: Created complete training script with MLflow integration.

\subsubsection{Issue 2: Path Resolution}
\textbf{Problem}: Training script couldn't find data file path.

\textbf{Solution}: Fixed project root path in DAG to \texttt{/opt/airflow} and ensured proper path handling.

\subsection{Commands Executed}
\begin{verbatim}
docker-compose exec airflow-scheduler python src/train.py \
    data/processed/stock_data_processed_20251126_192923.csv
\end{verbatim}

\newpage

\section{Step 11: Git Repository Management}

\subsection{Objective}
Commit all project files to Git repository and prepare for version control.

\subsection{Files Added to Git}
\begin{itemize}
    \item \texttt{.gitignore} - Git ignore patterns
    \item \texttt{.dockerignore} - Docker ignore patterns
    \item \texttt{requirements.txt} - Python dependencies
    \item \texttt{docker-compose.yml} - Docker Compose configuration
    \item \texttt{docker/Dockerfile} - Docker image definition
    \item \texttt{src/\_\_init\_\_.py} - Source package marker
    \item \texttt{src/config.py} - Configuration module
    \item \texttt{src/data\_extraction.py} - Data extraction script
    \item \texttt{src/data\_quality\_check.py} - Data quality validation
    \item \texttt{src/data\_transformation.py} - Data transformation script
    \item \texttt{src/train.py} - Model training script
    \item \texttt{airflow/dags/stock\_prediction\_dag.py} - Airflow DAG
\end{itemize}

\subsection{Files Excluded from Git}
\begin{itemize}
    \item \texttt{.env} - Environment variables (contains secrets)
    \item \texttt{data/raw/*.csv} - Raw data files (tracked by DVC)
    \item \texttt{data/processed/*.csv} - Processed data files (tracked by DVC)
    \item \texttt{models/*.pkl} - Model files (tracked by DVC)
    \item \texttt{airflow/logs/} - Airflow log files
    \item \texttt{airflow/airflow.db} - Airflow metadata database
    \item \texttt{airflow/airflow.cfg} - Airflow configuration (generated)
    \item \texttt{airflow-webserver.pid} - Process ID file
    \item \texttt{webserver\_config.py} - Generated webserver config
    \item \texttt{\_\_pycache\_\_/} - Python cache directories
    \item \texttt{*.pyc} - Compiled Python files
\end{itemize}

\subsection{Commands Executed}
\begin{verbatim}
git add .gitignore
git add .dockerignore
git add requirements.txt
git add docker-compose.yml
git add docker/Dockerfile
git add src/
git add airflow/dags/
git commit -m "Initial project setup"
git commit -m "Add data extraction module"
git commit -m "Add data quality check module"
git commit -m "Add data transformation module"
git commit -m "Add Docker configuration"
git commit -m "Add Airflow DAG"
git commit -m "Add model training script"
git commit -m "Update gitignore and clean up unnecessary files"
\end{verbatim}

\subsection{Cleanup Performed}
Removed unnecessary files and directories:
\begin{itemize}
    \item \texttt{airflow-webserver.pid}
    \item \texttt{webserver\_config.py}
    \item \texttt{airflow.cfg} (root directory duplicate)
    \item \texttt{dags/} (root directory duplicate)
    \item \texttt{logs/} (root directory duplicate)
    \item \texttt{plugins/} (root directory duplicate)
\end{itemize}

\newpage

\section{Complete File Structure}

\subsection{Final Project Structure}
\begin{verbatim}
MLOps-Stock-Prediction/
|-- .git/
|-- .gitignore
|-- .dockerignore
|-- .env                    (not in Git)
|-- requirements.txt
|-- docker-compose.yml
|-- docker/
|   `-- Dockerfile
|-- src/
|   |-- __init__.py
|   |-- config.py
|   |-- data_extraction.py
|   |-- data_quality_check.py
|   |-- data_transformation.py
|   `-- train.py
|-- airflow/
|   |-- dags/
|   |   `-- stock_prediction_dag.py
|   |-- logs/              (not in Git)
|   |-- plugins/
|   |-- airflow.db         (not in Git)
|   `-- airflow.cfg        (not in Git)
|-- data/
|   |-- raw/               (DVC tracked)
|   |   `-- stock_data_*.csv
|   `-- processed/         (DVC tracked)
|       |-- stock_data_processed_*.csv
|       `-- data_profile_report_*.html
|-- models/                (DVC tracked)
|   |-- stock_model.pkl
|   `-- feature_importance.csv
|-- tests/
|-- monitoring/
`-- MLOps_Project_Documentation.tex
\end{verbatim}

\newpage

\section{Data Flow Summary}

\subsection{Pipeline Flow}
\begin{enumerate}
    \item \textbf{Data Extraction} (\texttt{src/data\_extraction.py})
    \begin{itemize}
        \item Input: Alpha Vantage API
        \item Output: \texttt{data/raw/stock\_data\_\{SYMBOL\}\_\{TIMESTAMP\}.csv}
    \end{itemize}
    
    \item \textbf{Data Quality Check} (\texttt{src/data\_quality\_check.py})
    \begin{itemize}
        \item Input: Raw CSV file
        \item Output: Validation result (True/False)
    \end{itemize}
    
    \item \textbf{Data Transformation} (\texttt{src/data\_transformation.py})
    \begin{itemize}
        \item Input: Raw CSV file
        \item Output: 
        \begin{itemize}
            \item \texttt{data/processed/stock\_data\_processed\_\{SYMBOL\}\_\{TIMESTAMP\}.csv}
            \item \texttt{data/processed/data\_profile\_report\_\{TIMESTAMP\}.html}
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Data Versioning} (DVC with AWS S3)
    \begin{itemize}
        \item Input: Processed CSV and HTML report
        \item Output: 
        \begin{itemize}
            \item DVC tracked files (.dvc files) - committed to Git
            \item Actual data files - pushed to AWS S3 bucket
        \end{itemize}
        \item Remote Storage: AWS S3 bucket (\texttt{mlops-stock-prediction-data})
        \item Storage Path: \texttt{s3://mlops-stock-prediction-data/dvc-storage/}
    \end{itemize}
    
    \item \textbf{Model Training} (\texttt{src/train.py})
    \begin{itemize}
        \item Input: Processed CSV file
        \item Output:
        \begin{itemize}
            \item \texttt{models/stock\_model.pkl}
            \item \texttt{models/feature\_importance.csv}
            \item MLflow experiment run (logged to Dagshub)
        \end{itemize}
    \end{itemize}
\end{enumerate}

\newpage

\section{Key Commands Reference}

\subsection{Docker Commands}
\begin{verbatim}
# Build Docker images
docker-compose build --no-cache

# Initialize Airflow database
docker-compose up airflow-init

# Start all services
docker-compose up -d

# View logs
docker-compose logs airflow-scheduler
docker-compose logs airflow-webserver

# Execute commands in container
docker-compose exec airflow-scheduler bash
docker-compose exec airflow-scheduler python src/train.py <data_path>

# Stop services
docker-compose down

# Stop and remove volumes
docker-compose down -v
\end{verbatim}

\subsection{Airflow Commands}
\begin{verbatim}
# List DAGs
docker-compose exec airflow-scheduler airflow dags list

# Trigger DAG manually
docker-compose exec airflow-scheduler airflow dags trigger \
    stock_prediction_pipeline

# Check DAG status
docker-compose exec airflow-scheduler airflow dags show \
    stock_prediction_pipeline
\end{verbatim}

\subsection{Git Commands for Branching and PR Workflow}

\subsubsection{Basic Git Commands}
\begin{verbatim}
# Check current status
git status

# Check which branch you're on
git branch

# View all branches (local and remote)
git branch -a

# Check commit history
git log --oneline -10
git log --oneline --graph --all -10
\end{verbatim}

\subsubsection{Branching Commands}

\textbf{Create and Switch Branches}:
\begin{verbatim}
# Create new feature branch from dev
git checkout dev
git pull origin dev
git checkout -b feature/your-feature-name

# Switch to existing branch
git checkout dev
git checkout test
git checkout main

# Create branch from specific commit
git checkout -b feature/new-feature <commit-hash>
\end{verbatim}

\textbf{View Branch Status}:
\begin{verbatim}
# See which commits are ahead/behind
git log origin/main..origin/dev --oneline    # Commits in dev not in main
git log origin/dev..origin/main --oneline    # Commits in main not in dev

# Compare branches
git diff dev..main
git diff dev..test
\end{verbatim}

\subsubsection{Committing Changes}

\textbf{Standard Commit Workflow}:
\begin{verbatim}
# Stage specific files
git add src/config.py
git add .github/workflows/ci-dev-to-test.yml

# Stage all changes
git add -A
# OR
git add .

# Commit with message
git commit -m "fix: add AWS credentials to DVC pull step"

# Commit with detailed message
git commit -m "fix: add AWS credentials to DVC pull step

- Added AWS_ACCESS_KEY_ID to workflow environment
- Added AWS_SECRET_ACCESS_KEY to workflow environment
- Added AWS_DEFAULT_REGION for S3 access"
\end{verbatim}

\textbf{Commit Message Conventions}:
\begin{itemize}
    \item \texttt{feat:} - New feature
    \item \texttt{fix:} - Bug fix
    \item \texttt{docs:} - Documentation changes
    \item \texttt{test:} - Adding or updating tests
    \item \texttt{refactor:} - Code refactoring
    \item \texttt{chore:} - Maintenance tasks
\end{itemize}

\subsubsection{Pushing and Pulling}

\textbf{Push to Remote}:
\begin{verbatim}
# Push current branch to remote
git push origin feature/your-feature-name

# Push and set upstream
git push -u origin feature/your-feature-name

# Push all branches
git push --all origin

# Force push (use with caution!)
git push origin branch-name --force
\end{verbatim}

\textbf{Pull from Remote}:
\begin{verbatim}
# Pull latest changes
git pull origin dev

# Fetch without merging
git fetch origin

# Pull specific branch
git pull origin main
\end{verbatim}

\subsubsection{Complete PR Workflow Commands}

\textbf{PR \#1: Feature → Dev}

\begin{verbatim}
# Step 1: Start from dev branch
git checkout dev
git pull origin dev

# Step 2: Create feature branch
git checkout -b feature/new-feature

# Step 3: Make your changes
# Edit files, add code, etc.

# Step 4: Commit changes
git add .
git commit -m "feat: add new feature description"

# Step 5: Push feature branch
git push origin feature/new-feature

# Step 6: Create PR on GitHub
# Go to GitHub → Pull Requests → New PR
# Base: dev, Compare: feature/new-feature

# Step 7: After PR is merged, clean up
git checkout dev
git pull origin dev
git branch -d feature/new-feature  # Delete local branch
git push origin --delete feature/new-feature  # Delete remote branch
\end{verbatim}

\textbf{PR \#2: Dev → Test}

\begin{verbatim}
# Step 1: Ensure dev is up to date
git checkout dev
git pull origin dev

# Step 2: Create PR on GitHub
# Go to GitHub → Pull Requests → New PR
# Base: test, Compare: dev

# Step 3: Wait for workflow to complete
# - Model training runs automatically
# - CML comparison report is posted
# - Merge is blocked if model performs worse

# Step 4: After approval, merge PR on GitHub
# (Merge happens on GitHub, not via command line)

# Step 5: Update local test branch
git checkout test
git pull origin test
\end{verbatim}

\subsubsection{Why Does Pushing to Test Branch Automatically Run PR \#2 Workflow?}

\textbf{Understanding GitHub Actions Triggers}:

The PR \#2 workflow (\texttt{ci-dev-to-test.yml}) is configured with this trigger:

\begin{verbatim}
on:
  pull_request:
    branches:
      - test
    paths:
      - 'src/**'
      - 'airflow/**'
      - 'requirements.txt'
      - '.github/workflows/**'
\end{verbatim}

\textbf{What This Means}:

\begin{enumerate}
    \item \textbf{Trigger Event}: \texttt{pull\_request} - The workflow runs when a pull request event occurs
    
    \item \textbf{Target Branch}: \texttt{branches: - test} - The workflow only runs for PRs that target the \texttt{test} branch
    
    \item \textbf{Path Filters}: The workflow only runs if changes are made to:
    \begin{itemize}
        \item \texttt{src/**} - Any file in src directory
        \item \texttt{airflow/**} - Any file in airflow directory
        \item \texttt{requirements.txt} - Dependencies file
        \item \texttt{.github/workflows/**} - Workflow files
    \end{itemize}
\end{enumerate}

\textbf{When Does the Workflow Run Automatically?}

The workflow automatically runs in these situations:

\begin{enumerate}
    \item \textbf{When you create a PR targeting test branch}:
    \begin{itemize}
        \item Example: PR from \texttt{dev} → \texttt{test}
        \item GitHub detects: "New PR targeting test branch"
        \item Action: Workflow runs automatically
        \item Result: Model training starts, CML report is generated
    \end{itemize}
    
    \item \textbf{When you push new commits to an existing PR}:
    \begin{itemize}
        \item Example: PR \#2 is open, you push new commits to \texttt{dev}
        \item GitHub detects: "New commits in PR targeting test"
        \item Action: Workflow re-runs automatically
        \item Result: Model is retrained with new code changes
    \end{itemize}
    
    \item \textbf{When test branch is updated (re-validation)}:
    \begin{itemize}
        \item Example: PR \#3 merges \texttt{test} → \texttt{main}, then \texttt{test} gets new commits
        \item GitHub detects: "test branch was updated"
        \item Action: All open PRs targeting test are re-validated
        \item Result: PR \#2 workflow runs again (if PR is still open)
    \end{itemize}
    
    \item \textbf{When PR is reopened}:
    \begin{itemize}
        \item Example: PR was closed, then reopened
        \item GitHub detects: "PR reopened targeting test"
        \item Action: Workflow runs again
        \item Result: Fresh model training and comparison
    \end{itemize}
\end{enumerate}

\textbf{Why Only Test Branch?}

\begin{itemize}
    \item \textbf{Each workflow has a specific purpose}:
    \begin{itemize}
        \item PR \#1 workflow: Triggers on PRs to \texttt{dev} (code quality checks - fast, ~2 minutes)
        \item PR \#2 workflow: Triggers on PRs to \texttt{test} (model training and comparison - slower, ~10 minutes)
        \item PR \#3 workflow: Triggers on PRs to \texttt{main} (production deployment - final step)
    \end{itemize}
    
    \item \textbf{Separation of concerns}: Each branch has its own workflow
    \begin{itemize}
        \item \texttt{dev} branch: Code quality checks (linting, unit tests)
        \item \texttt{test} branch: Model training and comparison (ensures model quality before production)
        \item \texttt{main} branch: Production deployment (builds Docker image, pushes to registry)
    \end{itemize}
    
    \item \textbf{Efficiency}: 
    \begin{itemize}
        \item Model training is expensive (takes 5-10 minutes, uses API calls)
        \item We don't want to train models for every feature branch
        \item Only train when code reaches \texttt{test} branch (closer to production)
        \item This saves time and API quota
    \end{itemize}
    
    \item \textbf{Quality gate}:
    \begin{itemize}
        \item Code must pass quality checks (PR \#1) before model training
        \item Model must pass comparison (PR \#2) before deployment
        \item Deployment only happens (PR \#3) after all checks pass
    \end{itemize}
\end{itemize}

\textbf{What Happens When You Push Directly to Test Branch?}

If you push directly to \texttt{test} branch (not recommended):

\begin{itemize}
    \item \textbf{No PR exists}: Workflow won't run automatically
    \item \textbf{Why}: The workflow is configured for \texttt{pull\_request} events, not \texttt{push} events
    \item \textbf{Solution}: Always create a PR from \texttt{dev} → \texttt{test} to trigger the workflow
    \item \textbf{Best Practice}: Never push directly to \texttt{test} or \texttt{main} branches
\end{itemize}

\textbf{Summary}:

\begin{itemize}
    \item PR \#2 workflow runs \textbf{automatically} when:
    \begin{itemize}
        \item PR is created targeting \texttt{test} branch
        \item New commits are pushed to PR targeting \texttt{test}
        \item \texttt{test} branch is updated (re-validates open PRs)
        \item PR is reopened after being closed
    \end{itemize}
    
    \item It runs \textbf{only} for PRs targeting \texttt{test} branch
    
    \item It runs \textbf{only} if changes affect relevant paths (\texttt{src/}, \texttt{airflow/}, etc.)
    
    \item This ensures models are trained and compared before code reaches production
    
    \item This prevents bad models from reaching production automatically
\end{itemize}

\textbf{PR \#3: Test → Main}

\begin{verbatim}
# Step 1: Ensure test is up to date
git checkout test
git pull origin test

# Step 2: Create PR on GitHub
# Go to GitHub → Pull Requests → New PR
# Base: main, Compare: test

# Step 3: Wait for workflow to complete
# - Model fetched from MLflow
# - Docker image built
# - Image pushed to Docker Hub
# - Deployment verified

# Step 4: After approval, merge PR on GitHub
# (Merge happens on GitHub, not via command line)

# Step 5: Update local main branch
git checkout main
git pull origin main
\end{verbatim}

\subsubsection{Syncing Branches}

\textbf{Sync dev with main}:
\begin{verbatim}
git checkout dev
git pull origin main
git push origin dev
\end{verbatim}

\textbf{Sync feature branch with dev}:
\begin{verbatim}
git checkout feature/your-feature-name
git merge dev
# Resolve any conflicts if they occur
git push origin feature/your-feature-name
\end{verbatim}

\textbf{Sync all branches with main}:
\begin{verbatim}
# Sync dev
git checkout dev
git pull origin main
git push origin dev

# Sync test
git checkout test
git pull origin main
git push origin test

# Sync feature branch
git checkout feature/your-feature-name
git merge main
git push origin feature/your-feature-name
\end{verbatim}

\subsubsection{Merging and Resolving Conflicts}

\textbf{Merge Branches}:
\begin{verbatim}
# Merge dev into current branch
git checkout feature/your-feature-name
git merge dev

# Merge main into current branch
git merge main

# Merge with no fast-forward (creates merge commit)
git merge --no-ff dev
\end{verbatim}

\textbf{Resolve Merge Conflicts}:
\begin{verbatim}
# If conflicts occur during merge:
# 1. Git will show conflicted files
git status

# 2. Open conflicted files and resolve conflicts
# Look for <<<<<<< HEAD markers

# 3. After resolving, stage files
git add <resolved-file>

# 4. Complete the merge
git commit -m "merge: resolve conflicts with dev"
\end{verbatim}

\subsubsection{Useful Git Commands}

\textbf{Viewing History}:
\begin{verbatim}
# View commit history
git log --oneline -10
git log --oneline --graph --all -10

# View changes in a file
git log --follow -- src/config.py

# View what changed in a commit
git show <commit-hash>

# Compare two commits
git diff <commit1> <commit2>
\end{verbatim}

\textbf{Undoing Changes}:
\begin{verbatim}
# Discard changes in working directory
git restore <file>
git restore .

# Unstage files (keep changes)
git reset HEAD <file>

# Undo last commit (keep changes)
git reset --soft HEAD~1

# Undo last commit (discard changes)
git reset --hard HEAD~1
\end{verbatim}

\textbf{Stashing Changes}:
\begin{verbatim}
# Save changes temporarily
git stash

# List stashes
git stash list

# Apply stashed changes
git stash apply

# Apply and remove stash
git stash pop

# Delete stash
git stash drop
\end{verbatim}

\subsubsection{Important Notes}

\begin{itemize}
    \item \textbf{Never commit directly to main}: Always use PRs
    \item \textbf{Always pull before pushing}: \texttt{git pull} before \texttt{git push}
    \item \textbf{Create feature branches from dev}: Start from latest dev
    \item \textbf{Use descriptive commit messages}: Follow conventional commits
    \item \textbf{Keep branches synced}: Regularly sync with main to avoid conflicts
    \item \textbf{Delete merged branches}: Clean up after PRs are merged
\end{itemize}

\newpage

\section{Troubleshooting Guide}

\subsection{Common Issues and Solutions}

\subsubsection{Issue: DAG Not Appearing in Airflow UI}
\textbf{Symptoms}: DAG file exists but doesn't show in Airflow web UI.

\textbf{Solutions}:
\begin{enumerate}
    \item Verify DAG file is in correct location: \texttt{airflow/dags/}
    \item Check volume mount in \texttt{docker-compose.yml}: \texttt{./airflow/dags:/opt/airflow/dags}
    \item Verify DAG folder environment variable: \texttt{AIRFLOW\_\_CORE\_\_DAGS\_FOLDER}
    \item Check scheduler logs: \texttt{docker-compose logs airflow-scheduler}
    \item Ensure no syntax errors in DAG file
    \item Restart scheduler: \texttt{docker-compose restart airflow-scheduler}
\end{enumerate}

\subsubsection{Issue: Import Errors in DAG}
\textbf{Symptoms}: DAG fails with "ModuleNotFoundError".

\textbf{Solutions}:
\begin{enumerate}
    \item Verify project root path: \texttt{project\_root = Path('/opt/airflow')}
    \item Add to Python path: \texttt{sys.path.insert(0, str(project\_root))}
    \item Ensure \texttt{src/} directory is mounted in Docker
    \item Check PYTHONPATH environment variable in \texttt{docker-compose.yml}
\end{enumerate}

\subsubsection{Issue: API Rate Limits}
\textbf{Symptoms}: Alpha Vantage API returns error or 'Information' message.

\textbf{Solutions}:
\begin{enumerate}
    \item Use \texttt{outputsize='compact'} instead of 'full'
    \item Wait between API calls (free tier: 5 calls/minute)
    \item Check API key validity
    \item Consider upgrading to premium API key for higher limits
\end{enumerate}

\subsubsection{Issue: Docker Build Timeout}
\textbf{Symptoms}: Docker build fails with timeout errors.

\textbf{Solutions}:
\begin{enumerate}
    \item Increase pip timeout: \texttt{--timeout=300}
    \item Add retries: \texttt{--retries=5}
    \item Upgrade pip first: \texttt{pip install --upgrade pip setuptools wheel}
    \item Use \texttt{--no-cache-dir} to avoid cache issues
\end{enumerate}

\subsubsection{Issue: PostgreSQL Connection Error}
\textbf{Symptoms}: \texttt{airflow-init} fails to connect to PostgreSQL.

\textbf{Solutions}:
\begin{enumerate}
    \item Add \texttt{depends\_on} with \texttt{condition: service\_healthy}
    \item Verify PostgreSQL health check configuration
    \item Check network connectivity between containers
    \item Ensure PostgreSQL service starts before airflow-init
\end{enumerate}

\subsubsection{Issue: MLflow Connection Error}
\textbf{Symptoms}: Model training fails to log to MLflow.

\textbf{Solutions}:
\begin{enumerate}
    \item Verify MLFLOW\_TRACKING\_URI in \texttt{.env} file
    \item Check Dagshub credentials (username and token)
    \item Ensure network access to Dagshub
    \item Verify MLflow package is installed: \texttt{pip list | grep mlflow}
\end{enumerate}

\newpage

\section{Phase III: Continuous Integration \& Deployment (CI/CD)}

\subsection{Overview}
Phase III implements a complete CI/CD pipeline using GitHub Actions, following a strict branching model with automated checks, model training, comparison, and deployment. This phase ensures code quality, model performance validation, and automated production deployment.

\subsection{Branching Model}
The project follows a strict three-branch model:
\begin{itemize}
    \item \textbf{dev}: Development branch for feature integration
    \item \textbf{test}: Staging branch for model validation
    \item \textbf{main}: Production branch for stable releases
\end{itemize}

\textbf{Workflow}: Feature Branch → dev → test → main

\subsection{PR Workflow Overview}

\subsubsection{PR \#1: Feature → Dev}
\begin{itemize}
    \item \textbf{Trigger}: Pull request targeting \texttt{dev} branch
    \item \textbf{Workflow}: \texttt{ci-feature-to-dev.yml}
    \item \textbf{Actions}:
    \begin{enumerate}
        \item Code quality checks (Linting with flake8)
        \item Unit tests execution (pytest)
        \item Code coverage upload (Codecov)
    \end{enumerate}
    \item \textbf{Purpose}: Ensure code quality before merging to dev
    \item \textbf{Duration}: ~2-3 minutes
\end{itemize}

\subsubsection{PR \#2: Dev → Test}
\begin{itemize}
    \item \textbf{Trigger}: Pull request targeting \texttt{test} branch
    \item \textbf{Workflow}: \texttt{ci-dev-to-test.yml}
    \item \textbf{Actions}:
    \begin{enumerate}
        \item Pull latest data from DVC (AWS S3)
        \item Train new model with latest data
        \item Compare new model with production model using CML
        \item Post comparison report in PR comments
        \item Block merge if new model performs worse
    \end{enumerate}
    \item \textbf{Purpose}: Validate model performance before production
    \item \textbf{Duration}: ~10-15 minutes
    \item \textbf{Model Comparison}: Uses CML (Continuous Machine Learning) to compare RMSE and R² metrics
\end{itemize}

\subsubsection{PR \#3: Test → Main}
\begin{itemize}
    \item \textbf{Trigger}: Pull request targeting \texttt{main} branch
    \item \textbf{Workflow}: \texttt{cd-test-to-main.yml}
    \item \textbf{Actions}:
    \begin{enumerate}
        \item Fetch best model from MLflow Model Registry
        \item Build Docker image for FastAPI service
        \item Push image to Docker Hub
        \item Verify deployment (health check)
        \item Create deployment tag
    \end{enumerate}
    \item \textbf{Purpose}: Deploy validated model to production
    \item \textbf{Duration}: ~5-8 minutes
\end{itemize}

\subsection{Workflow Files Explained}

\subsubsection{.github/workflows/ci-feature-to-dev.yml}
\textbf{Purpose}: Validates code quality for feature branches merging into dev.

\textbf{Key Components}:
\begin{itemize}
    \item \textbf{Trigger}: PRs targeting \texttt{dev} branch
    \item \textbf{Path Filters}: Only runs when \texttt{src/}, \texttt{tests/}, or \texttt{requirements.txt} change
    \item \textbf{Steps}:
    \begin{enumerate}
        \item Checkout code
        \item Set up Python 3.8
        \item Cache pip packages for faster builds
        \item Install dependencies (flake8, pytest, pytest-cov)
        \item Run linting with flake8 (continues on error)
        \item Run unit tests with pytest (continues on error)
        \item Upload coverage to Codecov (continues on error)
    \end{enumerate}
    \item \textbf{Continue-on-error}: All steps continue on error to show all issues, not just the first
\end{itemize}

\textbf{Why This Workflow}:
\begin{itemize}
    \item Fast feedback loop (~2 minutes)
    \item Catches syntax errors and basic bugs early
    \item Ensures code quality before model training
    \item Non-blocking (allows merge even with warnings)
\end{itemize}

\subsubsection{.github/workflows/ci-dev-to-test.yml}
\textbf{Purpose}: Trains model and compares performance before allowing merge to test.

\textbf{Key Components}:
\begin{itemize}
    \item \textbf{Trigger}: PRs targeting \texttt{test} branch
    \item \textbf{Permissions}: Requires \texttt{pull-requests: write} and \texttt{issues: write} for CML comments
    \item \textbf{Steps}:
    \begin{enumerate}
        \item Checkout code and set up Python
        \item Install dependencies including DVC and boto3
        \item Configure AWS credentials for S3 access
        \item Pull latest data from DVC (AWS S3)
        \item Train model with latest data
        \item Set up Node.js and install CML
        \item Compare models and post report to PR
        \item Block merge if model performance degraded
    \end{enumerate}
    \item \textbf{Environment Variables}:
    \begin{itemize}
        \item \texttt{AWS\_ACCESS\_KEY\_ID}: For DVC S3 access
        \item \texttt{AWS\_SECRET\_ACCESS\_KEY}: For DVC S3 access
        \item \texttt{MLFLOW\_TRACKING\_URI}: MLflow server URL
        \item \texttt{DAGSHUB\_USERNAME}: Dagshub username
        \item \texttt{DAGSHUB\_TOKEN}: Dagshub authentication token
        \item \texttt{ALPHA\_VANTAGE\_KEY}: API key for data fetching
        \item \texttt{PYTHONPATH}: Set to workspace root for module imports
    \end{itemize}
\end{itemize}

\textbf{Model Comparison Logic}:
\begin{itemize}
    \item Fetches production model from MLflow Model Registry
    \item Trains new model with latest data
    \item Compares metrics: RMSE (lower is better) and R² (higher is better)
    \item Calculates percentage change
    \item Posts formatted report in PR comments using CML
    \item Exits with error if model degraded (blocks merge)
\end{itemize}

\textbf{Why This Workflow}:
\begin{itemize}
    \item Ensures model quality before production
    \item Prevents performance regression
    \item Provides transparent model comparison
    \item Blocks bad models automatically
\end{itemize}

\textbf{Why Only Test Branch Runs This}:
\begin{itemize}
    \item Model training is expensive (5-10 minutes, API calls)
    \item We don't train for every feature branch
    \item Only train when code reaches test (closer to production)
    \item Saves time and API quota
    \item Quality gate: Code must pass PR \#1 before model training
\end{itemize}

\subsubsection{.github/workflows/cd-test-to-main.yml}
\textbf{Purpose}: Deploys validated model to production.

\textbf{Key Components}:
\begin{itemize}
    \item \textbf{Trigger}: PRs targeting \texttt{main} branch
    \item \textbf{Steps}:
    \begin{enumerate}
        \item Checkout code and set up Python
        \item Install MLflow and dependencies
        \item Fetch best model from MLflow Model Registry
        \item Promote model to "Production" stage if in "Staging"
        \item Build Docker image using \texttt{docker/api/Dockerfile}
        \item Push image to Docker Hub with version tag
        \item Verify deployment (pull image, run container, health check)
        \item Create deployment tag
    \end{enumerate}
    \item \textbf{Docker Hub Configuration}:
    \begin{itemize}
        \item Repository: \texttt{\$DOCKER\_HUB\_USERNAME/mlops-stock-prediction-api}
        \item Tags: \texttt{latest} and \texttt{v\{timestamp\}}
        \item Credentials: \texttt{DOCKER\_HUB\_USERNAME} and \texttt{DOCKER\_HUB\_TOKEN} secrets
    \end{itemize}
\end{itemize}

\textbf{Model Registry Integration}:
\begin{itemize}
    \item Fetches model from MLflow Model Registry
    \item Prefers "Production" stage, falls back to "Staging"
    \item Automatically promotes Staging → Production on successful deployment
    \item Downloads model artifacts to \texttt{models/} directory
    \item Model is included in Docker image build
\end{itemize}

\textbf{Deployment Verification}:
\begin{itemize}
    \item Pulls built image from Docker Hub
    \item Runs container locally in CI
    \item Checks \texttt{/health} endpoint
    \item Verifies service starts correctly
    \item Creates deployment tag for tracking
\end{itemize}

\subsection{MLflow Model Registry}

\subsubsection{What is MLflow Model Registry?}
MLflow Model Registry is a centralized model store that provides:
\begin{itemize}
    \item \textbf{Model Versioning}: Track different versions of models
    \item \textbf{Model Staging}: Organize models by stage (Staging, Production, Archived)
    \item \textbf{Model Lineage}: Track which training run produced which model
    \item \textbf{Model Metadata}: Store model metrics, parameters, and artifacts
\end{itemize}

\subsubsection{Model Stages}
\begin{itemize}
    \item \textbf{Staging}: Models ready for testing, deployed to test environment
    \item \textbf{Production}: Models deployed to production, serving real traffic
    \item \textbf{Archived}: Old models no longer in use
\end{itemize}

\subsubsection{How Models are Registered}
\begin{enumerate}
    \item Model training script (\texttt{src/train.py}) logs model to MLflow
    \item Model is automatically registered with name: \texttt{stock\_prediction\_model}
    \item Model starts in "Staging" stage
    \item PR \#2 workflow trains model and registers it
    \item PR \#3 workflow promotes Staging → Production on successful deployment
\end{enumerate}

\subsubsection{Model Fetching in CD Pipeline}
\begin{verbatim}
# Fetch production model
client = mlflow.tracking.MlflowClient()
prod_versions = client.get_latest_versions(
    MODEL_NAME, 
    stages=["Production"]
)

# If no production model, use staging
if not prod_versions:
    prod_versions = client.get_latest_versions(
        MODEL_NAME, 
        stages=["Staging"]
    )

# Download model artifacts
mlflow.artifacts.download_artifacts(
    artifact_uri=f"models:/{MODEL_NAME}/{version}",
    dst_path="models"
)
\end{verbatim}

\subsection{Docker Hub Integration}

\subsubsection{What is Docker Hub?}
Docker Hub is a cloud-based registry service for Docker images:
\begin{itemize}
    \item Stores Docker images
    \item Provides image versioning
    \item Enables image sharing and distribution
    \item Used for production deployments
\end{itemize}

\subsubsection{Docker Hub Configuration}
\begin{itemize}
    \item \textbf{Repository}: \texttt{\{username\}/mlops-stock-prediction-api}
    \item \textbf{Image Tags}:
    \begin{itemize}
        \item \texttt{latest}: Always points to most recent production build
        \item \texttt{v\{timestamp\}}: Versioned tags for specific deployments
    \end{itemize}
    \item \textbf{Credentials}: Stored as GitHub Secrets
    \begin{itemize}
        \item \texttt{DOCKER\_HUB\_USERNAME}: Docker Hub username
        \item \texttt{DOCKER\_HUB\_TOKEN}: Docker Hub access token
    \end{itemize}
\end{itemize}

\subsubsection{Image Build Process}
\begin{enumerate}
    \item Dockerfile (\texttt{docker/api/Dockerfile}) defines image
    \item CD workflow builds image with model included
    \item Image tagged with version and "latest"
    \item Image pushed to Docker Hub
    \item Image can be pulled and deployed anywhere
\end{enumerate}

\subsection{FastAPI Service}

\subsubsection{src/api.py}
\textbf{Purpose}: REST API service for model serving.

\textbf{Key Features}:
\begin{itemize}
    \item \textbf{Endpoints}:
    \begin{itemize}
        \item \texttt{GET /}: API information
        \item \texttt{GET /health}: Health check endpoint
        \item \texttt{POST /predict}: Model prediction endpoint
        \item \texttt{GET /metrics}: Prometheus metrics endpoint (Phase IV)
    \end{itemize}
    \item \textbf{Model Loading}: Loads model from local \texttt{models/} directory or MLflow
    \item \textbf{Request Validation}: Uses Pydantic models for input validation
    \item \textbf{Error Handling}: Comprehensive error handling with HTTP status codes
    \item \textbf{Metrics}: Prometheus metrics integration (added in Phase IV)
\end{itemize}

\textbf{Prediction Endpoint}:
\begin{itemize}
    \item \textbf{Input}: JSON with \texttt{features} array (6 float values)
    \item \textbf{Output}: JSON with \texttt{prediction}, \texttt{model\_version}, \texttt{inference\_time\_ms}
    \item \textbf{Validation}: Validates feature count matches model expectations
    \item \textbf{Error Handling}: Returns 400 for invalid input, 500 for model errors
\end{itemize}

\subsubsection{docker/api/Dockerfile}
\textbf{Purpose}: Docker image definition for FastAPI service.

\textbf{Key Components}:
\begin{itemize}
    \item \textbf{Base Image}: \texttt{python:3.8-slim}
    \item \textbf{System Dependencies}: gcc (for compiling Python packages)
    \item \textbf{Python Dependencies}: Installed from \texttt{requirements.txt}
    \item \textbf{Optimizations}:
    \begin{itemize}
        \item \texttt{--prefer-binary}: Uses pre-built wheels (faster)
        \item \texttt{--no-cache-dir}: Reduces image size
        \item Removed duplicate \texttt{boto3} dependency (included in \texttt{dvc[s3]})
    \end{itemize}
    \item \textbf{Application Code}: Copies \texttt{src/} and \texttt{models/} directories
    \item \textbf{Port}: Exposes port 8000
    \item \textbf{Command}: Runs \texttt{uvicorn src.api:app --host 0.0.0.0 --port 8000}
\end{itemize}

\subsubsection{docker-compose.api.yml}
\textbf{Purpose}: Docker Compose configuration for API service with monitoring.

\textbf{Services}:
\begin{enumerate}
    \item \textbf{api}: FastAPI service
    \begin{itemize}
        \item Builds from \texttt{docker/api/Dockerfile}
        \item Exposes port 8000
        \item Mounts \texttt{models/} directory
        \item Health check: \texttt{/health} endpoint
    \end{itemize}
    \item \textbf{prometheus}: Metrics collection (Phase IV)
    \item \textbf{grafana}: Visualization dashboard (Phase IV)
\end{enumerate}

\subsection{Test Files}

\subsubsection{tests/test\_api.py}
\textbf{Purpose}: Unit tests for FastAPI endpoints.

\textbf{Test Cases}:
\begin{itemize}
    \item Health check endpoint test
    \item Prediction endpoint test with valid input
    \item Prediction endpoint test with invalid input
    \item Metrics endpoint test
\end{itemize}

\subsubsection{tests/test\_data\_quality.py}
\textbf{Purpose}: Data quality validation tests.

\textbf{Test Cases}:
\begin{itemize}
    \item Valid data validation
    \item Missing columns detection
    \item Invalid data types detection
\end{itemize}

\subsection{Common Issues and Solutions}

\subsubsection{Issue: ModuleNotFoundError in Workflows}
\textbf{Problem}: Python can't find local modules (\texttt{src/}).

\textbf{Solution}: Add \texttt{PYTHONPATH: \${{ github.workspace }}} to workflow environment.

\subsubsection{Issue: DVC Pull Fails}
\textbf{Problem}: DVC can't access AWS S3.

\textbf{Solution}: Add AWS credentials to workflow environment:
\begin{verbatim}
env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_DEFAULT_REGION: us-east-1
\end{verbatim}

\subsubsection{Issue: CML Can't Post PR Comments}
\textbf{Problem}: "Resource not accessible by integration" error.

\textbf{Solution}: Add permissions to workflow:
\begin{verbatim}
permissions:
  contents: read
  pull-requests: write
  issues: write
\end{verbatim}

\subsubsection{Issue: No Production Model Found}
\textbf{Problem}: First run has no production model.

\textbf{Solution}: Workflow falls back to "Staging" stage, then promotes to "Production" after successful deployment.

\subsubsection{Issue: API Rate Limit}
\textbf{Problem}: Alpha Vantage API rate limit exceeded.

\textbf{Solution}: Workflow detects rate limit and falls back to existing data if available.

\subsection{Required GitHub Secrets}
\begin{itemize}
    \item \texttt{AWS\_ACCESS\_KEY\_ID}: AWS S3 access key
    \item \texttt{AWS\_SECRET\_ACCESS\_KEY}: AWS S3 secret key
    \item \texttt{MLFLOW\_TRACKING\_URI}: MLflow server URL
    \item \texttt{DAGSHUB\_USERNAME}: Dagshub username
    \item \texttt{DAGSHUB\_TOKEN}: Dagshub authentication token
    \item \texttt{ALPHA\_VANTAGE\_KEY}: Alpha Vantage API key
    \item \texttt{DOCKER\_HUB\_USERNAME}: Docker Hub username
    \item \texttt{DOCKER\_HUB\_TOKEN}: Docker Hub access token
    \item \texttt{CODECOV\_TOKEN}: Codecov token (optional)
\end{itemize}

\newpage

\section{Phase IV: Monitoring and Observability}

\subsection{Overview}
Phase IV implements comprehensive monitoring and observability for the MLOps Stock Prediction system using Prometheus for metrics collection and Grafana for visualization and alerting. This phase ensures the model and API remain reliable in production.

\subsection{Architecture}

\begin{verbatim}
FastAPI API (Port 8000)
    ↓ (exposes /metrics endpoint)
Prometheus (Port 9090)
    ↓ (scrapes metrics every 15s)
Grafana (Port 3000)
    ↓ (visualizes metrics)
Dashboard & Alerts
\end{verbatim}

\subsection{Prometheus}

\subsubsection{What is Prometheus?}
Prometheus is an open-source monitoring and alerting toolkit:
\begin{itemize}
    \item \textbf{Data Collection}: Scrapes metrics from HTTP endpoints
    \item \textbf{Time-Series Database}: Stores metrics over time
    \item \textbf{Query Language}: PromQL for querying metrics
    \item \textbf{Alerting}: Can trigger alerts based on metrics
\end{itemize}

\subsubsection{monitoring/prometheus.yml}
\textbf{Purpose}: Prometheus configuration file.

\textbf{Key Components}:
\begin{itemize}
    \item \textbf{Global Settings}:
    \begin{itemize}
        \item \texttt{scrape\_interval: 15s}: How often to scrape targets
        \item \texttt{evaluation\_interval: 15s}: How often to evaluate alert rules
    \end{itemize}
    \item \textbf{Scrape Configurations}:
    \begin{enumerate}
        \item \textbf{Prometheus itself}: \texttt{localhost:9090}
        \item \textbf{FastAPI API}: \texttt{api:8000/metrics}
        \begin{itemize}
            \item Job name: \texttt{fastapi-api}
            \item Labels: \texttt{service: stock-prediction-api}, \texttt{component: api}
        \end{itemize}
    \end{enumerate}
\end{itemize}

\textbf{File Location}: \texttt{monitoring/prometheus.yml}

\textbf{Mount Point}: \texttt{/etc/prometheus/prometheus.yml} in Docker container

\subsection{Grafana}

\subsubsection{What is Grafana?}
Grafana is an open-source analytics and visualization platform:
\begin{itemize}
    \item \textbf{Dashboards}: Create visual dashboards for metrics
    \item \textbf{Data Sources}: Connect to Prometheus, databases, etc.
    \item \textbf{Alerting}: Configure alerts based on metrics
    \item \textbf{User-Friendly}: Web-based UI for monitoring
\end{itemize}

\subsubsection{monitoring/grafana/provisioning/datasources/prometheus.yml}
\textbf{Purpose}: Configures Prometheus as a data source in Grafana.

\textbf{Key Components}:
\begin{itemize}
    \item \texttt{apiVersion: 1}: Grafana provisioning API version
    \item \texttt{datasources}: List of data sources
    \item \textbf{Prometheus Configuration}:
    \begin{itemize}
        \item \texttt{name: Prometheus}: Display name
        \item \texttt{type: prometheus}: Data source type
        \item \texttt{url: http://prometheus:9090}: Prometheus service URL
        \item \texttt{isDefault: true}: Default data source
        \item \texttt{access: proxy}: Access mode
        \item \texttt{timeInterval: 15s}: Query interval
    \end{itemize}
\end{itemize}

\textbf{File Location}: \texttt{monitoring/grafana/provisioning/datasources/prometheus.yml}

\textbf{Mount Point}: \texttt{/etc/grafana/provisioning/datasources/} in Docker container

\subsubsection{monitoring/grafana/provisioning/dashboards/dashboard.yml}
\textbf{Purpose}: Configures dashboard provisioning in Grafana.

\textbf{Key Components}:
\begin{itemize}
    \item \texttt{apiVersion: 1}: Grafana provisioning API version
    \item \texttt{providers}: Dashboard providers configuration
    \item \textbf{Provider Settings}:
    \begin{itemize}
        \item \texttt{name: MLOps Stock Prediction}: Provider name
        \item \texttt{orgId: 1}: Organization ID
        \item \texttt{type: file}: Load dashboards from files
        \item \texttt{path: /etc/grafana/provisioning/dashboards}: Dashboard file location
        \item \texttt{updateIntervalSeconds: 10}: How often to check for updates
        \item \texttt{allowUiUpdates: true}: Allow manual dashboard edits
    \end{itemize}
\end{itemize}

\textbf{File Location}: \texttt{monitoring/grafana/provisioning/dashboards/dashboard.yml}

\textbf{Mount Point}: \texttt{/etc/grafana/provisioning/dashboards/} in Docker container

\subsubsection{monitoring/grafana/provisioning/dashboards/mlops-dashboard.json}
\textbf{Purpose}: Grafana dashboard definition with all panels and visualizations.

\textbf{Dashboard Panels}:
\begin{enumerate}
    \item \textbf{API Request Rate}: Line graph showing requests per second by endpoint
    \item \textbf{Inference Latency (p50, p95, p99)}: Line graph showing latency percentiles
    \item \textbf{Total API Requests/sec}: Single stat showing current request rate
    \item \textbf{Average Inference Latency (p50)}: Single stat showing average latency
    \item \textbf{Data Drift Count/sec}: Single stat showing drift detection rate
    \item \textbf{Total Predictions/sec}: Single stat showing prediction request rate
    \item \textbf{Data Drift Ratio}: Line graph showing percentage of requests with drift
    \item \textbf{Latency Distribution Heatmap}: Heatmap showing latency distribution over time
\end{enumerate}

\textbf{Queries Used}:
\begin{itemize}
    \item \texttt{rate(api\_requests\_total[5m])}: Request rate
    \item \texttt{histogram\_quantile(0.95, rate(api\_inference\_latency\_seconds\_bucket[5m]))}: p95 latency
    \item \texttt{rate(data\_drift\_detected\_total[5m]) / rate(prediction\_requests\_total[5m])}: Drift ratio
\end{itemize}

\textbf{File Location}: \texttt{monitoring/grafana/provisioning/dashboards/mlops-dashboard.json}

\subsubsection{monitoring/README.md}
\textbf{Purpose}: Comprehensive guide for monitoring setup and usage.

\textbf{Contents}:
\begin{itemize}
    \item Overview of monitoring architecture
    \item Setup instructions
    \item Access URLs for all services
    \item Metrics explanation
    \item Alerting configuration guide
    \item Troubleshooting section
    \item Customization guide
\end{itemize}

\subsection{Metrics Implementation}

\subsubsection{Service Metrics}
\textbf{API Request Count} (\texttt{api\_requests\_total}):
\begin{itemize}
    \item \textbf{Type}: Counter
    \item \textbf{Labels}: \texttt{method}, \texttt{endpoint}, \texttt{status}
    \item \textbf{Purpose}: Track total number of API requests
    \item \textbf{Example}: \texttt{api\_requests\_total\{method="POST",endpoint="/predict",status="200"\}}
\end{itemize}

\textbf{Inference Latency} (\texttt{api\_inference\_latency\_seconds}):
\begin{itemize}
    \item \textbf{Type}: Histogram
    \item \textbf{Buckets}: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0] seconds
    \item \textbf{Purpose}: Track prediction latency distribution
    \item \textbf{Use Case}: Calculate p50, p95, p99 percentiles
\end{itemize}

\subsubsection{Model/Data Drift Metrics}
\textbf{Data Drift Detections} (\texttt{data\_drift\_detected\_total}):
\begin{itemize}
    \item \textbf{Type}: Counter
    \item \textbf{Purpose}: Count requests with out-of-distribution features
    \item \textbf{Incremented}: When \texttt{detect\_data\_drift()} returns True
\end{itemize}

\textbf{Prediction Requests} (\texttt{prediction\_requests\_total}):
\begin{itemize}
    \item \textbf{Type}: Counter
    \item \textbf{Purpose}: Total prediction requests made
    \item \textbf{Use Case}: Calculate drift ratio: \texttt{drift\_detections / total\_requests}
\end{itemize}

\textbf{Data Drift Ratio}:
\begin{itemize}
    \item \textbf{Formula}: \texttt{rate(data\_drift\_detected\_total[5m]) / rate(prediction\_requests\_total[5m])}
    \item \textbf{Purpose}: Percentage of requests with data drift
    \item \textbf{Threshold}: Alert fires if ratio > 10\% for 5 minutes
\end{itemize}

\subsubsection{Data Drift Detection Logic}
\textbf{Function}: \texttt{detect\_data\_drift(features: np.ndarray) -> bool}

\textbf{Detection Rules}:
\begin{enumerate}
    \item \textbf{Negative Values}: If any feature < 0, drift detected
    \item \textbf{Extreme Values}: If any feature > 1e10, drift detected
    \item \textbf{Reason}: Stock prices and volumes should be positive and reasonable
\end{enumerate}

\textbf{Implementation Location}: \texttt{src/api.py}

\subsection{API Metrics Endpoint}

\subsubsection{GET /metrics}
\textbf{Purpose}: Exposes Prometheus-formatted metrics.

\textbf{Response Format}: Prometheus text format

\textbf{Metrics Exposed}:
\begin{itemize}
    \item \texttt{api\_requests\_total}: Request counter
    \item \texttt{api\_inference\_latency\_seconds\_bucket}: Latency histogram buckets
    \item \texttt{api\_inference\_latency\_seconds\_count}: Total latency observations
    \item \texttt{api\_inference\_latency\_seconds\_sum}: Sum of all latencies
    \item \texttt{data\_drift\_detected\_total}: Drift counter
    \item \texttt{prediction\_requests\_total}: Prediction counter
\end{itemize}

\textbf{Access}: \texttt{http://localhost:8000/metrics}

\subsection{Test Scripts}

\subsubsection{test\_data\_drift.ps1}
\textbf{Purpose}: PowerShell script to test data drift detection.

\textbf{Functionality}:
\begin{enumerate}
    \item Sends 20 prediction requests with drift (negative values)
    \item Waits 20 seconds for Prometheus to scrape
    \item Queries Prometheus for drift metrics
    \item Displays:
    \begin{itemize}
        \item Total predictions
        \item Drift detections
        \item Drift ratio percentage
    \end{itemize}
    \item Provides next steps for alert configuration
\end{enumerate}

\textbf{Usage}:
\begin{verbatim}
.\test_data_drift.ps1
\end{verbatim}

\textbf{What It Tests}:
\begin{itemize}
    \item Data drift detection is working
    \item Metrics are being collected
    \item Prometheus can query metrics
    \item Drift ratio calculation
\end{itemize}

\subsection{Alerting Configuration}

\subsubsection{Alert Rules}
Alerts are configured manually in Grafana UI (not via provisioning files due to format compatibility).

\textbf{Alert 1: High Inference Latency}
\begin{itemize}
    \item \textbf{Name}: High Inference Latency
    \item \textbf{Query}: \texttt{histogram\_quantile(0.95, rate(api\_inference\_latency\_seconds\_bucket[5m]))}
    \item \textbf{Condition}: > 0.5 seconds (500ms)
    \item \textbf{Duration}: 2 minutes
    \item \textbf{Severity}: Warning
    \item \textbf{Folder}: MLOps Alerts
    \item \textbf{Labels}: \texttt{severity=warning}, \texttt{service=stock-prediction-api}
\end{itemize}

\textbf{Alert 2: Data Drift Spike}
\begin{itemize}
    \item \textbf{Name}: Data Drift Spike
    \item \textbf{Query}: \texttt{(rate(data\_drift\_detected\_total[5m]) / rate(prediction\_requests\_total[5m]))}
    \item \textbf{Condition}: > 0.1 (10\%)
    \item \textbf{Duration}: 5 minutes
    \item \textbf{Severity}: Critical
    \item \textbf{Folder}: MLOps Alerts
    \item \textbf{Labels}: \texttt{severity=critical}, \texttt{service=stock-prediction-api}
\end{itemize}

\subsubsection{Alert Configuration Steps}
\begin{enumerate}
    \item Open Grafana → Alerting → Alert rules
    \item Click "New alert rule"
    \item Configure query, condition, and duration
    \item Set folder: "MLOps Alerts"
    \item Add labels: \texttt{severity}, \texttt{service}, \texttt{alert\_type}
    \item Set evaluation group: \texttt{mlops\_stock\_prediction\_alerts}
    \item Set evaluation interval: 30s
    \item Save rule
\end{enumerate}

\subsubsection{Notification Channels (Optional)}
\begin{itemize}
    \item \textbf{Slack}: Configure webhook URL in Grafana
    \item \textbf{Email}: Configure SMTP settings
    \item \textbf{File}: Log alerts to file (default)
    \item \textbf{Purpose}: Receive alerts when thresholds are exceeded
\end{itemize}

\textbf{Note}: Alerting requirement is met by having alerts configured in Grafana. Slack/file logging is optional enhancement.

\subsection{Docker Compose Configuration}

\subsubsection{docker-compose.api.yml}
\textbf{Purpose}: Orchestrates API, Prometheus, and Grafana services.

\textbf{Services}:
\begin{enumerate}
    \item \textbf{api}: FastAPI service
    \begin{itemize}
        \item Port: 8000
        \item Health check: \texttt{/health} endpoint
        \item Volumes: \texttt{models/} directory
    \end{itemize}
    \item \textbf{prometheus}: Metrics collection
    \begin{itemize}
        \item Port: 9090
        \item Config: \texttt{monitoring/prometheus.yml}
        \item Data volume: \texttt{prometheus-data}
        \item Retention: 30 days
    \end{itemize}
    \item \textbf{grafana}: Visualization
    \begin{itemize}
        \item Port: 3000
        \item Default login: \texttt{admin/admin}
        \item Provisioning: \texttt{monitoring/grafana/provisioning/}
        \item Data volume: \texttt{grafana-data}
    \end{itemize}
\end{enumerate}

\textbf{Networks}: All services on \texttt{monitoring} network

\textbf{Volumes}: Persistent storage for Prometheus and Grafana data

\subsection{Setup and Usage}

\subsubsection{Starting Services}
\begin{verbatim}
docker-compose -f docker-compose.api.yml up -d
\end{verbatim}

\subsubsection{Accessing Services}
\begin{itemize}
    \item \textbf{API}: http://localhost:8000
    \item \textbf{Prometheus}: http://localhost:9090
    \item \textbf{Grafana}: http://localhost:3000 (admin/admin)
\end{itemize}

\subsubsection{Generating Test Traffic}
\begin{verbatim}
# PowerShell
for ($i=1; $i -le 20; $i++) {
    $body = @{features=@(150.0, 155.0, 152.0, 155.0, 155.0, 150.0)} | ConvertTo-Json
    Invoke-RestMethod -Uri "http://localhost:8000/predict" -Method POST -ContentType "application/json" -Body $body
    Start-Sleep -Milliseconds 500
}
\end{verbatim}

\subsubsection{Testing Data Drift}
\begin{verbatim}
.\test_data_drift.ps1
\end{verbatim}

\subsection{File Structure}
\begin{verbatim}
monitoring/
|-- prometheus.yml                    # Prometheus configuration
|-- grafana/
|   `-- provisioning/
|       |-- datasources/
|       |   `-- prometheus.yml       # Prometheus datasource config
|       `-- dashboards/
|           |-- dashboard.yml        # Dashboard provisioning config
|           `-- mlops-dashboard.json # Dashboard definition
`-- README.md                         # Monitoring guide
\end{verbatim}

\subsection{Troubleshooting}

\subsubsection{Prometheus Not Scraping API}
\begin{enumerate}
    \item Check API is running: \texttt{docker-compose ps api}
    \item Check metrics endpoint: \texttt{curl http://localhost:8000/metrics}
    \item Check Prometheus targets: http://localhost:9090/targets
    \item Verify network connectivity between containers
\end{enumerate}

\subsubsection{Grafana Not Showing Data}
\begin{enumerate}
    \item Verify Prometheus datasource is configured
    \item Test datasource connection in Grafana
    \item Check time range in dashboard (use "Last 1 hour")
    \item Verify metrics exist in Prometheus
\end{enumerate}

\subsubsection{No Metrics Appearing}
\begin{enumerate}
    \item Generate API traffic (make prediction requests)
    \item Wait 15-30 seconds for Prometheus to scrape
    \item Check Prometheus query: \texttt{api\_requests\_total}
    \item Verify API metrics endpoint is accessible
\end{enumerate}

\newpage

\section{Summary}

\subsection{Project Completion Status (Phase 4 Complete)}
\begin{itemize}
    \item[$\checkmark$] \textbf{Phase I}: Project initialization and structure setup
    \item[$\checkmark$] \textbf{Phase I}: Environment configuration
    \item[$\checkmark$] \textbf{Phase I}: Source code modules creation
    \item[$\checkmark$] \textbf{Phase I}: Python dependencies setup
    \item[$\checkmark$] \textbf{Phase II}: Data extraction module
    \item[$\checkmark$] \textbf{Phase II}: Data quality check module
    \item[$\checkmark$] \textbf{Phase II}: Data transformation module
    \item[$\checkmark$] \textbf{Phase II}: Docker configuration
    \item[$\checkmark$] \textbf{Phase II}: Airflow DAG creation
    \item[$\checkmark$] \textbf{Phase II}: Model training module
    \item[$\checkmark$] \textbf{Phase II}: DVC remote storage configuration (AWS S3)
    \item[$\checkmark$] \textbf{Phase II}: Data versioning with cloud storage integration
    \item[$\checkmark$] \textbf{Phase III}: CI/CD pipeline with GitHub Actions
    \item[$\checkmark$] \textbf{Phase III}: FastAPI model serving API
    \item[$\checkmark$] \textbf{Phase III}: Docker containerization
    \item[$\checkmark$] \textbf{Phase III}: MLflow Model Registry integration
    \item[$\checkmark$] \textbf{Phase III}: Docker Hub deployment
    \item[$\checkmark$] \textbf{Phase III}: CML model comparison
    \item[$\checkmark$] \textbf{Phase IV}: Prometheus metrics collection
    \item[$\checkmark$] \textbf{Phase IV}: Grafana dashboards and visualization
    \item[$\checkmark$] \textbf{Phase IV}: Data drift detection and metrics
    \item[$\checkmark$] \textbf{Phase IV}: Alerting configuration
    \item[$\checkmark$] Git repository setup
    \item[$\checkmark$] Documentation
\end{itemize}

\subsection{Key Achievements}
\begin{enumerate}
    \item Successfully implemented end-to-end MLOps pipeline
    \item Integrated Alpha Vantage API for data extraction
    \item Implemented data quality checks and transformations
    \item Set up Apache Airflow for workflow orchestration
    \item Configured MLflow for experiment tracking
    \item Created Docker-based deployment environment
    \item Established proper version control practices with DVC
    \item Configured AWS S3 as cloud storage for large data files
    \item Successfully migrated from Google Drive to AWS S3 after overcoming multiple challenges
    \item Implemented automated data versioning with cloud storage push
    \item Implemented CI/CD pipeline with GitHub Actions (Phase III)
    \item Created FastAPI REST API for model serving (Phase III)
    \item Integrated MLflow Model Registry for model versioning (Phase III)
    \item Automated Docker image building and deployment (Phase III)
    \item Implemented CML for automated model comparison (Phase III)
    \item Set up Prometheus for metrics collection (Phase IV)
    \item Created Grafana dashboards for monitoring (Phase IV)
    \item Implemented data drift detection and alerting (Phase IV)
    \item Configured comprehensive monitoring and observability (Phase IV)
\end{enumerate}

\subsection{Next Steps (Future Work)}
\begin{itemize}
    \item Add more sophisticated feature engineering
    \item Implement advanced data drift detection (statistical tests)
    \item Set up automated retraining pipeline
    \item Implement A/B testing framework
    \item Add model explainability (SHAP values)
    \item Implement canary deployments
    \item Add more comprehensive alerting (Slack, Email integration)
    \item Scale to multiple models and services
    \item Implement cost monitoring and optimization
\end{itemize}

\newpage

\section{Appendix: File Reference Table}

\begin{longtable}{|p{3cm}|p{4cm}|p{7cm}|}
\hline
\textbf{File Name} & \textbf{Step Created} & \textbf{Purpose} \\
\hline
\endfirsthead

\hline
\textbf{File Name} & \textbf{Step Created} & \textbf{Purpose} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

.gitignore & Step 1 & Git ignore patterns for Python, data, models, Airflow files \\
\hline
.env & Step 2 & Environment variables for API keys and credentials (not in Git) \\
\hline
src/\_\_init\_\_.py & Step 3 & Python package marker for src directory \\
\hline
src/config.py & Step 3 & Centralized configuration for API keys, paths, MLflow settings \\
\hline
requirements.txt & Step 4 & Python package dependencies list \\
\hline
src/data\_extraction.py & Step 5 & Fetches stock data from Alpha Vantage API \\
\hline
src/data\_quality\_check.py & Step 6 & Validates data quality before processing \\
\hline
src/data\_transformation.py & Step 7 & Transforms data and creates features \\
\hline
docker/Dockerfile & Step 8 & Custom Docker image for Airflow with dependencies \\
\hline
docker-compose.yml & Step 8 & Docker Compose configuration for all services \\
\hline
.dockerignore & Step 8 & Files to exclude from Docker build context \\
\hline
airflow/dags/stock\_prediction\_dag.py & Step 9 & Airflow DAG orchestrating entire pipeline \\
\hline
src/train.py & Step 10 & Model training script with MLflow integration \\
\hline
.dvc/config & Step 11 & DVC remote storage configuration (AWS S3) \\
\hline
.dvc/config.local & Step 11 & DVC local configuration (Dagshub credentials) \\
\hline
data/raw/stock\_data\_*.csv & Step 5 (Generated) & Raw stock data from API \\
\hline
data/processed/stock\_data\_processed\_*.csv & Step 7 (Generated) & Processed data with features \\
\hline
data/processed/data\_profile\_report\_*.html & Step 7 (Generated) & Data profiling HTML report \\
\hline
data/processed/*.dvc & Step 11 (Generated) & DVC metadata files (tracked in Git) \\
\hline
models/stock\_model.pkl & Step 10 (Generated) & Trained machine learning model \\
\hline
models/feature\_importance.csv & Step 10 (Generated) & Feature importance rankings \\
\hline
.github/workflows/ci-feature-to-dev.yml & Phase III & CI workflow for feature → dev PRs \\
\hline
.github/workflows/ci-dev-to-test.yml & Phase III & CI workflow for dev → test PRs (model training) \\
\hline
.github/workflows/cd-test-to-main.yml & Phase III & CD workflow for test → main PRs (deployment) \\
\hline
src/api.py & Phase III & FastAPI REST API for model serving \\
\hline
docker/api/Dockerfile & Phase III & Dockerfile for FastAPI API service \\
\hline
docker-compose.api.yml & Phase III/IV & Docker Compose for API and monitoring services \\
\hline
tests/test\_api.py & Phase III & Unit tests for API endpoints \\
\hline
tests/test\_data\_quality.py & Phase III & Data quality validation tests \\
\hline
monitoring/prometheus.yml & Phase IV & Prometheus scrape configuration \\
\hline
monitoring/grafana/provisioning/datasources/prometheus.yml & Phase IV & Grafana Prometheus datasource config \\
\hline
monitoring/grafana/provisioning/dashboards/dashboard.yml & Phase IV & Grafana dashboard provisioning config \\
\hline
monitoring/grafana/provisioning/dashboards/mlops-dashboard.json & Phase IV & Grafana dashboard definition \\
\hline
monitoring/README.md & Phase IV & Monitoring setup and usage guide \\
\hline
test\_data\_drift.ps1 & Phase IV & PowerShell script for testing data drift detection \\
\hline

\end{longtable}

\end{document}

