\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{amssymb}

\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\rhead{MLOps Stock Prediction Project}
\lhead{Complete Technical Guide}
\cfoot{\thepage}

% Define professional colors
\definecolor{primaryblue}{RGB}{0,51,102}
\definecolor{secondaryblue}{RGB}{0,102,204}
\definecolor{accentgold}{RGB}{255,204,0}
\definecolor{darkgray}{RGB}{64,64,64}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Code listing style
\lstset{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

% Define custom languages for listings
\lstdefinelanguage{yaml}{
    keywords={true,false,null,y,n},
    keywordstyle=\color{magenta}\bfseries,
    basicstyle=\ttfamily\footnotesize,
    sensitive=false,
    comment=[l]{\#},
    morecomment=[s]{/*}{*/},
    commentstyle=\color{codegreen}\ttfamily,
    stringstyle=\color{codepurple}\ttfamily,
    morestring=[b]',
    morestring=[b]"
}

\lstdefinelanguage{dockerfile}{
    keywords={FROM,RUN,CMD,LABEL,EXPOSE,ENV,ADD,COPY,ENTRYPOINT,VOLUME,USER,WORKDIR,ARG,ONBUILD},
    keywordstyle=\color{magenta}\bfseries,
    basicstyle=\ttfamily\footnotesize,
    commentstyle=\color{codegreen}\ttfamily,
    stringstyle=\color{codepurple}\ttfamily
}

\lstdefinelanguage{PowerShell}{
    keywords={if,else,for,foreach,while,do,function,return,break,continue},
    keywordstyle=\color{magenta}\bfseries,
    basicstyle=\ttfamily\footnotesize,
    commentstyle=\color{codegreen}\ttfamily,
    stringstyle=\color{codepurple}\ttfamily,
    morecomment=[l]{\#}
}

\title{
    \vspace{-1.5cm}
    \centering
    \textcolor{primaryblue}{\rule{\textwidth}{3pt}}\\[0.5cm]
    \textcolor{primaryblue}{\textbf{\Huge MLOps Complete Technical Guide}}\\[0.8cm]
    \textcolor{secondaryblue}{\textbf{\Large From Code to Production}}\\[0.4cm]
    \textcolor{darkgray}{\textbf{\large Stock Prediction MLOps Pipeline}}\\[0.6cm]
    \textcolor{darkgray}{\large Comprehensive Step-by-Step Documentation}\\[0.5cm]
    \textcolor{primaryblue}{\rule{\textwidth}{3pt}}\\[1cm]
}
\author{
    \centering
    \begin{tabular}{l}
        \textcolor{primaryblue}{\textbf{\large Group Members:}}\\[0.3cm]
        \textcolor{darkgray}{1. Ahsan Faraz \hspace{2cm} i228791} \\[0.2cm]
        \textcolor{darkgray}{2. Gulsher Khan \hspace{2cm} i222637} \\[0.2cm]
        \textcolor{darkgray}{3. Muhammad Faisal \hspace{1.5cm} i228758} \\[0.8cm]
        \textcolor{primaryblue}{\textbf{\large Submitted To:}}\\[0.3cm]
        \textcolor{darkgray}{Sir Pir Samiullah Shah}\\[0.8cm]
        \textcolor{primaryblue}{\textbf{\large Institution:}}\\[0.3cm]
        \textcolor{darkgray}{FAST National University of Computer}\\
        \textcolor{darkgray}{and Emerging Sciences}
    \end{tabular}
}
\date{\vspace{0.8cm}\centering\textcolor{darkgray}{\today}}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={MLOps Complete Technical Guide}
}

\begin{document}

% Title Page
\begin{titlepage}
\centering
\maketitle
\thispagestyle{empty}
\vfill
\begin{center}
\textcolor{secondaryblue}{\rule{0.8\textwidth}{1pt}}\\[0.5cm]
\textcolor{darkgray}{\textit{This document provides a comprehensive technical guide explaining every phase of the MLOps pipeline, how components connect, which files handle each process, and detailed explanations of all tools and technologies used.}}\\[0.5cm]
\textcolor{secondaryblue}{\rule{0.8\textwidth}{1pt}}\\[0.8cm]
\textcolor{primaryblue}{\textbf{\large Project Links:}}\\[0.4cm]
\textcolor{secondaryblue}{\href{https://github.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction.git}{\textbf{GitHub Repository}}}\\[0.2cm]
\textcolor{secondaryblue}{\href{https://dagshub.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction}{\textbf{DAGsHub Project}}}
\end{center}
\vfill
\end{titlepage}

% Table of Contents
\newpage
\tableofcontents
\newpage

\section{Introduction}

This document provides a comprehensive technical guide to the MLOps Stock Prediction Pipeline. It explains how all four phases connect, which files handle each process, detailed explanations of tools and technologies, and step-by-step commands for every operation.

\subsection{Project Overview}

The MLOps pipeline consists of four interconnected phases:
\begin{enumerate}
    \item \textbf{Phase I: Project Initialization} - Setup and configuration
    \item \textbf{Phase II: Data Pipeline \& Training} - Data extraction, transformation, versioning, and model training
    \item \textbf{Phase III: CI/CD Pipeline} - Automated testing, model comparison, and deployment
    \item \textbf{Phase IV: Monitoring \& Observability} - Real-time metrics, dashboards, and alerting
\end{enumerate}

\subsection{How Phases Connect}

\begin{itemize}
    \item \textbf{Phase I → Phase II}: Initialized project structure enables data pipeline execution
    \item \textbf{Phase II → Phase III}: Trained models and versioned data enable CI/CD automation
    \item \textbf{Phase III → Phase IV}: Deployed production API enables monitoring and observability
    \item \textbf{Phase IV → Phase II}: Monitoring feedback informs model retraining decisions
\end{itemize}

\section{Phase I: Project Initialization}

\subsection{Overview}

Phase I establishes the foundation for the entire MLOps pipeline. It involves setting up version control, project structure, Python environment, and configuration files.

\subsection{Components and Files}

\subsubsection{Git Version Control}

\textbf{Purpose}: Track code changes, enable collaboration, and maintain project history.

\textbf{Files Involved}:
\begin{itemize}
    \item \texttt{.git/} - Git repository metadata
    \item \texttt{.gitignore} - Specifies files to exclude from version control
\end{itemize}

\textbf{Key Commands}:
\begin{lstlisting}[language=bash]
# Initialize Git repository
git init

# Add remote repository
git remote add origin https://github.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction.git

# Create branches
git checkout -b dev
git checkout -b test
git checkout -b feature/feature-name

# Commit changes
git add .
git commit -m "Initial project setup"

# Push to remote
git push origin main
\end{lstlisting}

\textbf{In Our Project}:
\begin{itemize}
    \item Main branch: Production-ready code
    \item Dev branch: Development work
    \item Test branch: Staging environment
    \item Feature branches: Individual features
\end{itemize}

\subsubsection{Python Project Structure}

\textbf{Purpose}: Organize code into logical modules for maintainability.

\textbf{Files Created}:
\begin{itemize}
    \item \texttt{src/} - Source code directory
    \item \texttt{src/\_\_init\_\_.py} - Makes src a Python package
    \item \texttt{src/config.py} - Configuration management
    \item \texttt{tests/} - Unit tests directory
    \item \texttt{requirements.txt} - Python dependencies
\end{itemize}

\textbf{Key Commands}:
\begin{lstlisting}[language=bash]
# Create project structure
mkdir -p src tests data/raw data/processed models

# Install dependencies
pip install -r requirements.txt

# Run tests
pytest tests/ -v
\end{lstlisting}

\subsubsection{Configuration Management}

\textbf{File}: \texttt{src/config.py}

\textbf{Purpose}: Centralize all configuration settings including API keys, paths, and MLflow settings.

\textbf{Key Configuration Variables}:
\begin{itemize}
    \item \texttt{ALPHA\_VANTAGE\_KEY} - API key for Alpha Vantage
    \item \texttt{MLFLOW\_TRACKING\_URI} - DAGsHub MLflow tracking URL
    \item \texttt{DAGSHUB\_USERNAME} - DAGsHub username
    \item \texttt{DAGSHUB\_TOKEN} - DAGsHub authentication token
    \item \texttt{DATA\_RAW\_DIR} - Path to raw data directory
    \item \texttt{DATA\_PROCESSED\_DIR} - Path to processed data directory
    \item \texttt{MODELS\_DIR} - Path to models directory
\end{itemize}

\textbf{How It Works}:
\begin{itemize}
    \item Uses \texttt{python-dotenv} to load environment variables from \texttt{.env} file
    \item Provides default values for optional settings
    \item Centralizes all configuration to avoid hardcoding
\end{itemize}

\subsection{Phase I Summary}

Phase I establishes:
\begin{itemize}
    \item Git repository with branching strategy
    \item Python project structure
    \item Configuration management system
    \item Development environment setup
\end{itemize}

\textbf{Connection to Phase II}: The initialized project structure enables the data pipeline scripts to run and store data in organized directories.

\section{Phase II: Data Pipeline \& Training}

\subsection{Overview}

Phase II implements the complete data pipeline from extraction to model training. It uses Apache Airflow for orchestration, DVC for data versioning, and MLflow for experiment tracking.

\subsection{Data Extraction}

\subsubsection{Alpha Vantage API}

\textbf{What is Alpha Vantage?}: A financial data API provider offering free and premium stock market data.

\textbf{Purpose}: Fetch real-time and historical stock price data.

\textbf{File}: \texttt{src/data\_extraction.py}

\textbf{How It Works}:
\begin{enumerate}
    \item Makes HTTP GET request to Alpha Vantage API endpoint
    \item Requests intraday data (60-minute intervals) for specified stock symbol
    \item Receives JSON response with time series data
    \item Converts JSON to pandas DataFrame
    \item Saves raw data as CSV file with timestamp
\end{enumerate}

\textbf{Key Code Sections}:
\begin{lstlisting}[language=Python]
# API endpoint
url = 'https://www.alphavantage.co/query'

# API parameters
params = {
    'function': 'TIME_SERIES_INTRADAY',
    'symbol': symbol,  # e.g., 'AAPL'
    'interval': '60min',
    'apikey': api_key,
    'outputsize': 'compact',  # Last 100 data points
    'datatype': 'json'
}

# Make request
response = requests.get(url, params=params, timeout=30)
data = response.json()

# Extract time series
time_series = data.get('Time Series (60min)', {})

# Convert to DataFrame
df = pd.DataFrame.from_dict(time_series, orient='index')

# Save to CSV
output_path = output_dir / f'stock_data_{symbol}_{timestamp}.csv'
df.to_csv(output_path)
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Run extraction manually
python src/data_extraction.py

# With custom symbol
python src/data_extraction.py AAPL YOUR_API_KEY
\end{lstlisting}

\textbf{Output}: Raw CSV files in \texttt{data/raw/} directory with naming pattern:
\texttt{stock\_data\_AAPL\_20251126\_192914.csv}

\subsubsection{Apache Airflow Orchestration}

\textbf{What is Apache Airflow?}: An open-source platform for programmatically authoring, scheduling, and monitoring workflows.

\textbf{Purpose}: Orchestrate the entire data pipeline automatically on a schedule.

\textbf{File}: \texttt{airflow/dags/stock\_prediction\_dag.py}

\textbf{How It Works}:
\begin{enumerate}
    \item DAG (Directed Acyclic Graph) defines workflow tasks
    \item Tasks execute in sequence: Extract → Quality Check → Transform → Version → Train
    \item XCom (cross-communication) passes data between tasks
    \item Scheduler runs DAG on schedule (daily in our case)
    \item Web UI provides monitoring and manual trigger capability
\end{enumerate}

\textbf{Key DAG Structure}:
\begin{lstlisting}[language=Python]
# Create DAG
dag = DAG(
    'stock_prediction_pipeline',
    description='ETL and ML pipeline',
    schedule='@daily',  # Run daily at midnight
    catchup=False,
)

# Define tasks
extract = PythonOperator(
    task_id='extract_data',
    python_callable=extract_task,
    dag=dag,
)

quality_check = PythonOperator(
    task_id='data_quality_check',
    python_callable=quality_check_task,
    dag=dag,
)

# Set dependencies
extract >> quality_check >> transform >> dvc_version >> train_model
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Start Airflow webserver
airflow webserver --port 8080

# Start Airflow scheduler
airflow scheduler

# Trigger DAG manually
airflow dags trigger stock_prediction_pipeline

# Check DAG status
airflow dags list
\end{lstlisting}

\textbf{In Our Project}:
\begin{itemize}
    \item DAG runs daily at midnight
    \item Tasks pass file paths via XCom
    \item Each task logs to Airflow logs directory
    \item Failed tasks can be retried automatically
\end{itemize}

\subsection{Data Quality Check}

\textbf{File}: \texttt{src/data\_quality\_check.py}

\textbf{Purpose}: Validate data before transformation to ensure data integrity.

\textbf{Checks Performed}:
\begin{itemize}
    \item Missing values check (threshold: 1\% null values)
    \item Data type validation
    \item Value range validation (prices > 0, volumes > 0)
    \item Duplicate detection
\end{itemize}

\textbf{How It Works}:
\begin{lstlisting}[language=Python]
def check_data_quality(file_path):
    df = pd.read_csv(file_path)
    
    # Check null values
    null_ratio = df.isnull().sum() / len(df)
    if null_ratio.max() > NULL_THRESHOLD:
        return False
    
    # Check value ranges
    if (df[['open', 'high', 'low', 'close']] < 0).any().any():
        return False
    
    return True
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Run quality check manually
python src/data_quality_check.py data/raw/stock_data_AAPL_*.csv
\end{lstlisting}

\subsection{Data Transformation}

\textbf{File}: \texttt{src/data\_transformation.py}

\textbf{Purpose}: Transform raw data into features suitable for machine learning.

\textbf{Transformations Applied}:
\begin{enumerate}
    \item \textbf{Lag Features}: Previous hour values (1h, 2h, 3h lag)
    \item \textbf{Rolling Statistics}: Moving averages and standard deviations
    \item \textbf{Time-based Features}: Hour of day, day of week
    \item \textbf{Target Variable}: Next hour's closing price
\end{enumerate}

\textbf{Key Code}:
\begin{lstlisting}[language=Python]
# Create lag features
df['close_lag_1h'] = df['close'].shift(1)
df['close_lag_2h'] = df['close'].shift(2)
df['close_lag_3h'] = df['close'].shift(3)

# Rolling statistics
df['close_rolling_mean_3h'] = df['close'].rolling(window=3).mean()
df['close_rolling_std_3h'] = df['close'].rolling(window=3).std()

# Time features
df['hour'] = df.index.hour
df['day_of_week'] = df.index.dayofweek

# Target variable (next hour's close)
df['target'] = df['close'].shift(-1)
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Run transformation manually
python src/data_transformation.py data/raw/stock_data_AAPL_*.csv
\end{lstlisting}

\textbf{Output}: Processed CSV files in \texttt{data/processed/} directory with features ready for training.

\textbf{Data Profiling Report}:
\begin{itemize}
    \item Generates HTML report using \texttt{ydata-profiling} (replacement for deprecated \texttt{pandas-profiling})
    \item Report includes: data quality summary, feature statistics, correlations, missing values analysis
    \item Saved as: \texttt{data\_profile\_report\_\{timestamp\}.html}
    \item \textbf{Logged to MLflow}: The profiling report is automatically logged as an artifact to MLflow Tracking Server (DAGsHub) in the Airflow DAG transform task
    \item Artifact path in MLflow: \texttt{data\_profiles/}
    \item This satisfies the requirement: "Documentation Artifact: Use Pandas Profiling or a similar tool to generate a detailed data quality and feature summary report. This report must be logged as an artifact to your MLflow Tracking Server (Dagshub)."
\end{itemize}

\subsection{Data Versioning with DVC}

\subsubsection{What is DVC?}

\textbf{DVC (Data Version Control)}: A version control system for machine learning projects that tracks data files, models, and experiments.

\textbf{Why Use DVC?}
\begin{itemize}
    \item \textbf{Large Files}: Git cannot efficiently handle large data files
    \item \textbf{Reproducibility}: Track which data version was used for each model
    \item \textbf{Collaboration}: Share data versions without storing in Git
    \item \textbf{Storage Efficiency}: Store data in cloud storage (S3, GCS, etc.)
\end{itemize}

\subsubsection{How DVC Works}

\textbf{File}: \texttt{.dvc/config}

\textbf{Configuration}:
\begin{lstlisting}
[core]
    remote = myremote
['remote "myremote"']
    url = s3://mlops-stock-prediction-data/dvc-storage
    region = us-east-1
['remote "origin"']
    url = https://dagshub.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction.dvc
\end{lstlisting}

\textbf{How It Works}:
\begin{enumerate}
    \item DVC creates a \texttt{.dvc} file (small text file) that tracks the data file
    \item The actual data file is stored in DVC cache locally
    \item When pushing, data is uploaded to remote storage (AWS S3 or DAGsHub)
    \item Git tracks only the \texttt{.dvc} file (small, text-based)
    \item When pulling, DVC downloads data from remote storage
\end{enumerate}

\textbf{Key Commands}:
\begin{lstlisting}[language=bash]
# Initialize DVC repository
dvc init

# Add remote storage (AWS S3)
dvc remote add -d myremote s3://bucket-name/dvc-storage

# Configure AWS credentials
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
export AWS_DEFAULT_REGION=us-east-1

# Add file to DVC tracking
dvc add data/processed/stock_data_processed_20251126.csv

# This creates: data/processed/stock_data_processed_20251126.csv.dvc

# Commit DVC file to Git
git add data/processed/stock_data_processed_20251126.csv.dvc
git commit -m "Add processed data version"

# Push data to remote storage
dvc push

# Pull data from remote storage
dvc pull

# Check DVC status
dvc status
\end{lstlisting}

\subsubsection{AWS S3 Storage}

\textbf{What is AWS S3?}: Amazon Simple Storage Service - object storage service for storing and retrieving data.

\textbf{Purpose in Our Project}: Store versioned data files remotely.

\textbf{How We Use It}:
\begin{itemize}
    \item DVC remote configured to use S3 bucket: \texttt{s3://mlops-stock-prediction-data/dvc-storage}
    \item When \texttt{dvc push} runs, data files are uploaded to S3
    \item When \texttt{dvc pull} runs, data files are downloaded from S3
    \item S3 provides scalable, durable storage for large data files
\end{itemize}

\textbf{Setup Commands}:
\begin{lstlisting}[language=bash]
# Create S3 bucket (via AWS CLI)
aws s3 mb s3://mlops-stock-prediction-data

# Configure DVC remote
dvc remote add -d myremote s3://mlops-stock-prediction-data/dvc-storage

# Set AWS credentials
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=us-east-1
\end{lstlisting}

\textbf{In Airflow DAG}: The \texttt{dvc\_version} task automatically:
\begin{enumerate}
    \item Adds processed data file to DVC tracking
    \item Commits \texttt{.dvc} file to Git
    \item Pushes data to S3 remote storage
    \item Falls back to DAGsHub remote if S3 fails
\end{enumerate}

\subsubsection{DAGsHub Storage}

\textbf{What is DAGsHub?}: A platform for data science and ML projects that integrates Git, DVC, and MLflow.

\textbf{Purpose in Our Project}:
\begin{itemize}
    \item \textbf{Alternative Remote}: Backup remote for DVC data storage
    \item \textbf{MLflow Hosting}: Hosts MLflow tracking server
    \item \textbf{Project Hub}: Centralized location for code, data, and experiments
\end{itemize}

\textbf{How We Use It}:
\begin{itemize}
    \item DVC remote \texttt{origin} points to DAGsHub: \texttt{https://dagshub.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction.dvc}
    \item MLflow tracking URI: \texttt{https://dagshub.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction.mlflow}
    \item Provides web UI for viewing experiments and data versions
\end{itemize}

\textbf{Setup}:
\begin{lstlisting}[language=bash]
# DAGsHub remote is configured in .dvc/config
# Authentication via DAGSHUB_TOKEN environment variable
export DAGSHUB_TOKEN=your_dagshub_token
\end{lstlisting}

\subsection{Model Training}

\textbf{File}: \texttt{src/train.py}

\textbf{Purpose}: Train machine learning model using processed data and log experiments to MLflow.

\subsubsection{MLflow}

\textbf{What is MLflow?}: An open-source platform for managing the ML lifecycle, including experimentation, reproducibility, and deployment.

\textbf{Key Components}:
\begin{itemize}
    \item \textbf{Tracking}: Log parameters, metrics, and artifacts
    \item \textbf{Projects}: Package ML code for reproducibility
    \item \textbf{Models}: Model packaging format
    \item \textbf{Model Registry}: Centralized model store
\end{itemize}

\textbf{How We Use MLflow}:
\begin{enumerate}
    \item Start MLflow run for each training execution
    \item Log hyperparameters (n\_estimators, max\_depth, etc.)
    \item Log metrics (RMSE, MAE, R²)
    \item Log model artifacts (model file, feature importance)
    \item Register model in Model Registry
    \item Transition model to Staging stage
\end{enumerate}

\textbf{Key Code}:
\begin{lstlisting}[language=Python]
import mlflow
import mlflow.sklearn

# Set tracking URI (DAGsHub)
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# Set credentials
os.environ['MLFLOW_TRACKING_USERNAME'] = DAGSHUB_USERNAME
os.environ['MLFLOW_TRACKING_PASSWORD'] = DAGSHUB_TOKEN

# Start run
with mlflow.start_run(run_name=run_name):
    # Log parameters
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 10)
    
    # Train model
    model = RandomForestRegressor(...)
    model.fit(X_train, y_train)
    
    # Calculate metrics
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    test_r2 = r2_score(y_test, y_pred)
    
    # Log metrics
    mlflow.log_metric("test_rmse", test_rmse)
    mlflow.log_metric("test_r2", test_r2)
    
    # Log model
    mlflow.sklearn.log_model(model, "model")
    
    # Register model
    model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
    registered_model = mlflow.register_model(
        model_uri=model_uri,
        name=MODEL_NAME
    )
    
    # Transition to Staging
    client.transition_model_version_stage(
        name=MODEL_NAME,
        version=registered_model.version,
        stage="Staging"
    )
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Train model manually
python src/train.py data/processed/stock_data_processed_20251126.csv

# View MLflow UI (if running locally)
mlflow ui --port 5000

# Access DAGsHub MLflow
# https://dagshub.com/ahsanfaraz-WebDev/MLOps-Stock-Prediction
\end{lstlisting}

\textbf{Model Registry Stages}:
\begin{itemize}
    \item \textbf{None}: Initial stage after registration
    \item \textbf{Staging}: Model ready for testing (set after training)
    \item \textbf{Production}: Model deployed to production (set in CI/CD)
    \item \textbf{Archived}: Deprecated models
\end{itemize}

\subsection{Phase II Summary}

Phase II implements:
\begin{itemize}
    \item Automated data extraction from Alpha Vantage API
    \item Data quality validation
    \item Feature engineering and transformation
    \item Data versioning with DVC (stored in AWS S3)
    \item Model training with MLflow tracking
    \item Model registration in MLflow Model Registry
\end{itemize}

\textbf{Connection to Phase III}: Trained models and versioned data enable CI/CD automation for testing and deployment.

\section{Phase III: CI/CD Pipeline}

\subsection{Overview}

Phase III implements Continuous Integration (CI) and Continuous Deployment (CD) using GitHub Actions. It automates testing, model comparison, and deployment.

\subsection{CI/CD Concepts}

\subsubsection{Continuous Integration (CI)}

\textbf{What is CI?}: Practice of automatically testing code changes when they are committed to a repository.

\textbf{Purpose}: Catch bugs early and ensure code quality.

\subsubsection{Continuous Deployment (CD)}

\textbf{What is CD?}: Practice of automatically deploying code changes to production after passing tests.

\textbf{Purpose}: Reduce manual deployment effort and enable rapid iteration.

\subsection{Branching Strategy}

\textbf{Our Branching Model}:
\begin{itemize}
    \item \textbf{main}: Production code
    \item \textbf{test}: Staging environment
    \item \textbf{dev}: Development branch
    \item \textbf{feature/*}: Feature branches
\end{itemize}

\textbf{Workflow}:
\begin{enumerate}
    \item Feature branch → Dev (PR \#1)
    \item Dev → Test (PR \#2)
    \item Test → Main (PR \#3)
\end{enumerate}

\subsubsection{PR Approval Requirements}

\textbf{Mandatory Requirement}: All Pull Requests to \texttt{test} and \texttt{main} branches must be approved by at least one peer before merging.

\textbf{GitHub Branch Protection Setup}:
\begin{enumerate}
    \item Navigate to: \texttt{Repository Settings → Branches}
    \item Add branch protection rule for \texttt{test} branch:
    \begin{itemize}
        \item \textbf{Require pull request reviews before merging}: Enabled
        \item \textbf{Required number of approvals}: 1
        \item \textbf{Dismiss stale pull request approvals when new commits are pushed}: Enabled
        \item \textbf{Require status checks to pass before merging}: Enabled (CI workflows)
    \end{itemize}
    \item Add branch protection rule for \texttt{main} branch:
    \begin{itemize}
        \item \textbf{Require pull request reviews before merging}: Enabled
        \item \textbf{Required number of approvals}: 1
        \item \textbf{Require status checks to pass before merging}: Enabled (CD workflow)
        \item \textbf{Require branches to be up to date before merging}: Enabled
    \end{itemize}
\end{enumerate}

\textbf{How It Works}:
\begin{itemize}
    \item When PR \#2 (dev → test) is created, it requires at least 1 approval
    \item When PR \#3 (test → main) is created, it requires at least 1 approval
    \item CI/CD workflows must pass before merge is allowed
    \item Ensures code quality and peer review before production deployment
\end{itemize}

\textbf{Commands} (via GitHub CLI):
\begin{lstlisting}[language=bash]
# Set branch protection (requires GitHub CLI)
gh api repos/:owner/:repo/branches/test/protection \
  --method PUT \
  --field required_pull_request_reviews[required_approving_review_count]=1 \
  --field required_status_checks[strict]=true

gh api repos/:owner/:repo/branches/main/protection \
  --method PUT \
  --field required_pull_request_reviews[required_approving_review_count]=1 \
  --field required_status_checks[strict]=true
\end{lstlisting}

\subsection{PR \#1: Feature → Dev}

\textbf{File}: \texttt{.github/workflows/ci-feature-to-dev.yml}

\textbf{Purpose}: Lint code and run unit tests when PR is created from feature branch to dev.

\textbf{Trigger}: Pull request to \texttt{dev} branch

\textbf{Steps}:
\begin{enumerate}
    \item Checkout code
    \item Set up Python 3.8
    \item Install dependencies
    \item Run flake8 linter
    \item Run pytest unit tests
    \item Upload coverage reports
\end{enumerate}

\textbf{Key Code}:
\begin{lstlisting}[language=yaml]
name: CI - Feature to Dev

on:
  pull_request:
    branches:
      - dev
    paths:
      - 'src/**'
      - 'tests/**'
      - 'requirements.txt'

jobs:
  lint-and-test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'
    
    - name: Install dependencies
      run: |
        pip install flake8 pytest pytest-cov
        pip install -r requirements.txt
    
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127
    
    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=src --cov-report=xml
\end{lstlisting}

\textbf{What Happens}:
\begin{itemize}
    \item GitHub Actions runner checks out code
    \item Installs Python dependencies
    \item Runs flake8 to check code style
    \item Runs pytest to execute unit tests
    \item Generates coverage report
    \item PR status shows pass/fail
\end{itemize}

\subsection{PR \#2: Dev → Test}

\textbf{File}: \texttt{.github/workflows/ci-dev-to-test.yml}

\textbf{Purpose}: Train model, compare with production model, and post CML report.

\textbf{Why Only Test Branch Triggers PR \#2?}

The workflow is configured to trigger only on PRs to the \texttt{test} branch because:
\begin{itemize}
    \item \textbf{Staging Environment}: Test branch represents staging where we validate models
    \item \textbf{Resource Efficiency}: Model training is expensive; we don't train on every dev PR
    \item \textbf{Quality Gate}: Only well-tested code reaches test branch
    \item \textbf{Model Comparison}: We compare against production model before promoting to main
\end{itemize}

\textbf{Trigger}: Pull request to \texttt{test} branch

\textbf{Steps}:
\begin{enumerate}
    \item Checkout code
    \item Set up Python 3.8
    \item Configure Git and DVC
    \item Configure AWS credentials
    \item Pull latest data from DVC (S3)
    \item Train model
    \item Compare model with production model
    \item Post CML report to PR
    \item Block merge if model performance degraded
\end{enumerate}

\textbf{Key Code - Model Training}:
\begin{lstlisting}[language=yaml]
- name: Train model
  env:
    MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
    DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
    DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
    ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
  run: |
    # Find latest processed data
    LATEST_DATA=$(ls -t data/processed/stock_data_processed_*.csv | head -1)
    
    # Train model
    python src/train.py "$LATEST_DATA"
\end{lstlisting}

\textbf{Key Code - Model Comparison}:
\begin{lstlisting}[language=Python]
# Get current model metrics
latest_run = client.search_runs(...)[0]
current_test_rmse = latest_run.data.metrics.get('test_rmse')

# Get production model metrics
prod_versions = client.get_latest_versions(MODEL_NAME, stages=["Production"])
if prod_versions:
    prod_run = client.get_run(prod_versions[0].run_id)
    prod_test_rmse = prod_run.data.metrics.get('test_rmse')
    
    # Compare
    if current_test_rmse < prod_test_rmse:
        print("✅ Model improved")
    else:
        print("❌ Model degraded - Blocking merge")
        exit(1)
\end{lstlisting}

\subsubsection{CML (Continuous Machine Learning)}

\textbf{What is CML?}: A tool for integrating ML workflows into CI/CD pipelines.

\textbf{Purpose}: Post model comparison reports directly to GitHub PRs.

\textbf{How We Use It}:
\begin{lstlisting}[language=bash]
# Install CML
npm install -g @dvcorg/cml

# Generate report
cat > report.md << EOF
## Model Performance Comparison

### Current Model
- Test RMSE: 2.3456
- Test R²: 0.8765

### Production Model
- Test RMSE: 2.4567
- Test R²: 0.8543

✅ Model Improvement: RMSE improved by 4.5%
EOF

# Post to PR
cml comment create report.md
\end{lstlisting}

\textbf{What Happens}:
\begin{itemize}
    \item Model is trained with latest data
    \item Metrics are compared with production model
    \item CML report is posted as PR comment
    \item If model degraded, merge is blocked
    \item If model improved, merge is allowed
\end{itemize}

\subsection{PR \#3: Test → Main}

\textbf{File}: \texttt{.github/workflows/cd-test-to-main.yml}

\textbf{Purpose}: Fetch production model, build Docker image, push to Docker Hub, and deploy.

\textbf{Trigger}: Pull request to \texttt{main} branch

\textbf{Steps}:
\begin{enumerate}
    \item Checkout code
    \item Set up Python
    \item Fetch best model from MLflow Model Registry
    \item Promote model to Production stage
    \item Build Docker image
    \item Push to Docker Hub
    \item Verify deployment
    \item Create deployment tag
\end{enumerate}

\textbf{Key Code - Model Fetching}:
\begin{lstlisting}[language=Python]
# Get production model from Model Registry
prod_versions = client.get_latest_versions(MODEL_NAME, stages=["Production"])

if prod_versions:
    model_version = prod_versions[0]
    
    # Download model
    mlflow.artifacts.download_artifacts(
        artifact_uri=f"models:/{MODEL_NAME}/{model_version.version}",
        dst_path="models"
    )
    
    # Promote to Production if in Staging
    if model_version.current_stage != "Production":
        client.transition_model_version_stage(
            name=MODEL_NAME,
            version=model_version.version,
            stage="Production"
        )
\end{lstlisting}

\subsubsection{Docker}

\textbf{What is Docker?}: Platform for containerizing applications.

\textbf{Purpose}: Package API with all dependencies for consistent deployment.

\textbf{File}: \texttt{docker/api/Dockerfile}

\textbf{Dockerfile Contents}:
\begin{lstlisting}[language=dockerfile]
FROM python:3.8-slim

WORKDIR /app

# Copy requirements
COPY requirements.txt .

# Install dependencies
RUN pip install --no-cache-dir --prefer-binary -r requirements.txt

# Copy source code
COPY src/ ./src/
COPY models/ ./models/

# Expose port
EXPOSE 8000

# Run API
CMD ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]
\end{lstlisting}

\textbf{Key Code - Docker Build \& Push}:
\begin{lstlisting}[language=yaml]
- name: Build Docker image
  run: |
    docker build -f docker/api/Dockerfile \
      -t ${{ secrets.DOCKER_HUB_USERNAME }}/mlops-stock-prediction-api:latest .
    docker build -f docker/api/Dockerfile \
      -t ${{ secrets.DOCKER_HUB_USERNAME }}/mlops-stock-prediction-api:${{ github.sha }} .

- name: Push Docker image to Docker Hub
  run: |
    docker push ${{ secrets.DOCKER_HUB_USERNAME }}/mlops-stock-prediction-api:latest
    docker push ${{ secrets.DOCKER_HUB_USERNAME }}/mlops-stock-prediction-api:${{ github.sha }}
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Build Docker image locally
docker build -f docker/api/Dockerfile -t mlops-api:latest .

# Run container
docker run -p 8000:8000 mlops-api:latest

# Push to Docker Hub
docker push username/mlops-stock-prediction-api:latest
\end{lstlisting}

\subsubsection{Docker Hub}

\textbf{What is Docker Hub?}: Cloud-based registry service for Docker images.

\textbf{Purpose}: Store and distribute Docker images.

\textbf{How We Use It}:
\begin{itemize}
    \item Build Docker image in CI/CD
    \item Push image to Docker Hub with tags (latest, commit SHA)
    \item Production servers pull image from Docker Hub
    \item Enables consistent deployment across environments
\end{itemize}

\subsection{Phase III Summary}

Phase III implements:
\begin{itemize}
    \item Automated linting and testing (PR \#1)
    \item Model training and comparison (PR \#2)
    \item CML reporting for model metrics
    \item Model deployment via Docker (PR \#3)
    \item Docker image publishing to Docker Hub
\end{itemize}

\textbf{Connection to Phase IV}: Deployed production API enables monitoring and observability.

\section{Phase IV: Monitoring \& Observability}

\subsection{Overview}

Phase IV implements comprehensive monitoring using Prometheus for metrics collection and Grafana for visualization and alerting.

\subsection{Prometheus}

\subsubsection{What is Prometheus?}

\textbf{Prometheus} is an open-source monitoring and alerting toolkit designed for reliability and scalability.

\textbf{Key Features}:
\begin{itemize}
    \item \textbf{Time-Series Database}: Stores metrics as time-series data
    \item \textbf{Pull Model}: Scrapes metrics from targets at regular intervals
    \item \textbf{Query Language}: PromQL for querying metrics
    \item \textbf{Alerting}: Can trigger alerts based on metric thresholds
\end{itemize}

\subsubsection{What is a Time-Series Database?}

\textbf{Time-Series Database}: A database optimized for storing and querying data points indexed by time.

\textbf{Characteristics}:
\begin{itemize}
    \item Data points have timestamps
    \item Efficient storage of sequential data
    \item Fast queries over time ranges
    \item Optimized for append operations
\end{itemize}

\textbf{Example}:
\begin{lstlisting}
# Time-series data
timestamp: 2025-12-01 10:00:00, value: 150.5
timestamp: 2025-12-01 10:01:00, value: 151.2
timestamp: 2025-12-01 10:02:00, value: 150.8
\end{lstlisting}

\textbf{Why Prometheus Uses Time-Series DB}:
\begin{itemize}
    \item Metrics are naturally time-series (values change over time)
    \item Efficient storage and querying of historical metrics
    \item Enables trend analysis and anomaly detection
\end{itemize}

\subsubsection{Prometheus Configuration}

\textbf{File}: \texttt{monitoring/prometheus.yml}

\textbf{Configuration}:
\begin{lstlisting}[language=yaml]
global:
  scrape_interval: 15s  # Scrape every 15 seconds
  evaluation_interval: 15s

scrape_configs:
  # Scrape Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Scrape FastAPI API service
  - job_name: 'fastapi-api'
    metrics_path: '/metrics'
    static_configs:
      - targets: ['api:8000']
        labels:
          service: 'stock-prediction-api'
          component: 'api'
\end{lstlisting}

\textbf{How It Works}:
\begin{enumerate}
    \item Prometheus scrapes \texttt{http://api:8000/metrics} every 15 seconds
    \item FastAPI exposes metrics at \texttt{/metrics} endpoint
    \item Prometheus stores metrics in its time-series database
    \item Metrics can be queried using PromQL
\end{enumerate}

\subsubsection{FastAPI Metrics Endpoint}

\textbf{File}: \texttt{src/api.py}

\textbf{Metrics Exposed}:
\begin{itemize}
    \item \texttt{api\_requests\_total}: Total API requests (counter)
    \item \texttt{api\_inference\_latency\_seconds}: Inference latency (histogram)
    \item \texttt{data\_drift\_detected\_total}: Data drift detections (counter)
    \item \texttt{prediction\_requests\_total}: Total prediction requests (counter)
\end{itemize}

\textbf{Key Code}:
\begin{lstlisting}[language=Python]
from prometheus_client import Counter, Histogram, generate_latest

# Define metrics
REQUEST_COUNT = Counter(
    'api_requests_total',
    'Total number of API requests',
    ['method', 'endpoint', 'status']
)

INFERENCE_LATENCY = Histogram(
    'api_inference_latency_seconds',
    'API inference latency in seconds',
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]
)

DATA_DRIFT_COUNTER = Counter(
    'data_drift_detected_total',
    'Total number of data drift detections'
)

PREDICTION_REQUESTS = Counter(
    'prediction_requests_total',
    'Total number of prediction requests'
)

# Metrics endpoint
@app.get("/metrics")
async def metrics():
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)

# Record metrics in prediction endpoint
@app.post("/predict")
async def predict(request: PredictionRequest):
    start_time = time.time()
    
    # Increment prediction counter
    PREDICTION_REQUESTS.inc()
    
    # Check for drift
    drift_detected = detect_data_drift(features_array)
    if drift_detected:
        DATA_DRIFT_COUNTER.inc()
    
    # Make prediction
    prediction = model.predict(features_array)[0]
    
    # Record latency
    inference_time = time.time() - start_time
    INFERENCE_LATENCY.observe(inference_time)
    
    # Increment request counter
    REQUEST_COUNT.labels(method='POST', endpoint='/predict', status='200').inc()
    
    return PredictionResponse(...)
\end{lstlisting}

\textbf{Data Drift Detection}:
\begin{lstlisting}[language=Python]
def detect_data_drift(features: np.ndarray) -> bool:
    """
    Simple data drift detection
    Checks if features are outside reasonable ranges
    """
    # Check for negative values (invalid for stock prices)
    if np.any(features < 0):
        return True
    
    # Check for extreme values
    if np.any(features > 1e10):
        return True
    
    return False
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# View metrics endpoint
curl http://localhost:8000/metrics

# Query Prometheus
curl http://localhost:9090/api/v1/query?query=api_requests_total
\end{lstlisting}

\subsection{Grafana}

\subsubsection{What is Grafana?}

\textbf{Grafana} is an open-source analytics and visualization platform.

\textbf{Purpose}:
\begin{itemize}
    \item Visualize metrics from Prometheus
    \item Create dashboards for monitoring
    \item Set up alerts based on metric thresholds
    \item Analyze trends and patterns
\end{itemize}

\textbf{Why Use Grafana?}
\begin{itemize}
    \item \textbf{Visualization}: Beautiful graphs and charts
    \item \textbf{Dashboards}: Customizable monitoring dashboards
    \item \textbf{Alerting}: Notify when metrics exceed thresholds
    \item \textbf{Integration}: Works with Prometheus and other data sources
\end{itemize}

\subsubsection{Grafana Configuration}

\textbf{File}: \texttt{monitoring/grafana/provisioning/datasources/prometheus.yml}

\textbf{Datasource Configuration}:
\begin{lstlisting}[language=yaml]
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
\end{lstlisting}

\textbf{How It Works}:
\begin{enumerate}
    \item Grafana connects to Prometheus as datasource
    \item Queries Prometheus using PromQL
    \item Displays metrics in dashboards
    \item Evaluates alert rules based on metrics
\end{enumerate}

\subsubsection{Grafana Dashboard}

\textbf{File}: \texttt{monitoring/grafana/provisioning/dashboards/mlops-dashboard.json}

\textbf{Dashboard Panels}:
\begin{enumerate}
    \item \textbf{API Request Rate}: Requests per second
    \item \textbf{Inference Latency}: p50, p95, p99 percentiles
    \item \textbf{Total API Requests}: Cumulative request count
    \item \textbf{Average Inference Latency}: Mean latency
    \item \textbf{Data Drift Count}: Number of drift detections
    \item \textbf{Total Predictions}: Cumulative prediction count
    \item \textbf{Data Drift Ratio}: Ratio of drift to total predictions
    \item \textbf{Latency Distribution Heatmap}: Latency distribution over time
\end{enumerate}

\textbf{Key PromQL Queries}:
\begin{lstlisting}
# API Request Rate
rate(api_requests_total[5m])

# Inference Latency (p50)
histogram_quantile(0.50, rate(api_inference_latency_seconds_bucket[5m]))

# Inference Latency (p95)
histogram_quantile(0.95, rate(api_inference_latency_seconds_bucket[5m]))

# Data Drift Ratio
rate(data_drift_detected_total[5m]) / rate(prediction_requests_total[5m])

# Total Predictions per second
sum(rate(prediction_requests_total[5m]))
\end{lstlisting}

\textbf{How Graphs Are Made}:
\begin{enumerate}
    \item Grafana queries Prometheus using PromQL
    \item Prometheus returns time-series data
    \item Grafana renders data as graphs/charts
    \item Dashboard refreshes automatically (every 10 seconds)
\end{enumerate}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Access Grafana UI
# http://localhost:3000
# Login: admin / admin (default)

# Dashboard is auto-loaded from:
# monitoring/grafana/provisioning/dashboards/mlops-dashboard.json
\end{lstlisting}

\subsubsection{Alerting}

\textbf{What is Alerting?}: Automated notifications when metrics exceed thresholds.

\textbf{Why Alerting?}
\begin{itemize}
    \item \textbf{Proactive Monitoring}: Detect issues before users report them
    \item \textbf{Performance Degradation}: Alert when latency increases
    \item \textbf{Data Quality}: Alert when data drift spikes
    \item \textbf{Availability}: Alert when service is down
\end{itemize}

\textbf{Alert Rules} (Configured manually in Grafana UI):

\begin{enumerate}
    \item \textbf{High Inference Latency}
    \begin{itemize}
        \item Condition: \texttt{api\_inference\_latency\_seconds > 0.5}
        \item Threshold: 500ms
        \item Action: Log alert (can be extended to Slack/email)
    \end{itemize}
    
    \item \textbf{Data Drift Spike}
    \begin{itemize}
        \item Condition: \texttt{data\_drift\_ratio > 0.1}
        \item Threshold: 10\% drift ratio
        \item Action: Log alert
    \end{itemize}
    
    \item \textbf{High Error Rate}
    \begin{itemize}
        \item Condition: \texttt{rate(api\_requests\_total\{status="500"\}[5m]) > 0.05}
        \item Threshold: 5\% error rate
        \item Action: Log alert
    \end{itemize}
\end{enumerate}

\textbf{How to Configure Alerts in Grafana}:
\begin{enumerate}
    \item Go to Grafana UI: \texttt{http://localhost:3000}
    \item Navigate to: \texttt{Alerting → Alert rules}
    \item Click: \texttt{Create alert rule}
    \item Set:
    \begin{itemize}
        \item \textbf{Folder}: Create or select folder
        \item \textbf{Evaluation group}: Set group name
        \item \textbf{Query}: PromQL query (e.g., \texttt{api\_inference\_latency\_seconds})
        \item \textbf{Condition}: Threshold (e.g., \texttt{> 0.5})
        \item \textbf{Evaluation interval}: How often to check (e.g., 1 minute)
    \end{itemize}
    \item Configure notification channel (optional):
    \begin{itemize}
        \item Slack webhook
        \item Email
        \item File logging
    \end{itemize}
    \item Save alert rule
\end{enumerate}

\textbf{Alert Configuration Example}:
\begin{lstlisting}[language=yaml]
# Example alert rule (conceptual)
alert:
  name: High Inference Latency
  condition: api_inference_latency_seconds > 0.5
  evaluation_interval: 1m
  notification:
    type: log
    # Can be extended to Slack/email
\end{lstlisting}

\textbf{Note}: Alert rules are configured manually in Grafana UI because automatic provisioning requires folder configuration which can be complex. The dashboard JSON file contains the visualization panels, but alerts are set up through the UI.

\subsection{Docker Compose Setup}

\textbf{File}: \texttt{docker-compose.api.yml}

\textbf{Purpose}: Orchestrate FastAPI, Prometheus, and Grafana services.

\textbf{Configuration}:
\begin{lstlisting}[language=yaml]
services:
  api:
    build:
      context: .
      dockerfile: docker/api/Dockerfile
    ports:
      - "8000:8000"
    networks:
      - monitoring

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - monitoring
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Start all services
docker-compose -f docker-compose.api.yml up -d

# View logs
docker-compose -f docker-compose.api.yml logs -f

# Stop services
docker-compose -f docker-compose.api.yml down

# Access services
# API: http://localhost:8000
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000
\end{lstlisting}

\subsection{Testing Monitoring}

\textbf{File}: \texttt{test\_data\_drift.ps1}

\textbf{Purpose}: Generate test traffic and verify metrics.

\textbf{How It Works}:
\begin{enumerate}
    \item Sends prediction requests with negative values (triggers drift)
    \item Waits for Prometheus to scrape metrics
    \item Queries Prometheus API for drift metrics
    \item Displays total predictions, drift detections, and drift ratio
\end{enumerate}

\textbf{Key Code}:
\begin{lstlisting}[language=PowerShell]
# Send drift requests
for ($i=1; $i -le 20; $i++) {
    $body = @{features=@(-10.0, 155.0, 152.0, 155.0, 155.0, 150.0)} | ConvertTo-Json
    Invoke-RestMethod -Uri "http://localhost:8000/predict" -Method POST -Body $body
}

# Query Prometheus
$drift = (Invoke-WebRequest -Uri "http://localhost:9090/api/v1/query?query=data_drift_detected_total").Content | ConvertFrom-Json
$total = (Invoke-WebRequest -Uri "http://localhost:9090/api/v1/query?query=prediction_requests_total").Content | ConvertFrom-Json

# Calculate ratio
$driftValue = [int]$drift.data.result[0].value[1]
$totalValue = [int]$total.data.result[0].value[1]
$ratio = ($driftValue / $totalValue) * 100
\end{lstlisting}

\textbf{Commands}:
\begin{lstlisting}[language=bash]
# Run test script (PowerShell)
.\test_data_drift.ps1

# Or generate traffic manually
for i in {1..10}; do
  curl -X POST http://localhost:8000/predict \
    -H "Content-Type: application/json" \
    -d '{"features": [150.0, 155.0, 148.0, 152.0, 1000000.0, 0.5]}'
done
\end{lstlisting}

\subsection{Phase IV Summary}

Phase IV implements:
\begin{itemize}
    \item Prometheus metrics collection from FastAPI
    \item Time-series database for metric storage
    \item Grafana dashboards for visualization
    \item PromQL queries for metric analysis
    \item Alert rules for proactive monitoring
    \item Data drift detection and tracking
    \item Inference latency monitoring
    \item Request rate tracking
\end{itemize}

\textbf{Connection Back to Phase II}: Monitoring feedback can inform when to retrain models (e.g., high drift ratio indicates need for retraining).

\section{Complete Workflow Summary}

\subsection{End-to-End Flow}

\begin{enumerate}
    \item \textbf{Phase I}: Project initialized with Git, Python, and configuration
    \item \textbf{Phase II}: 
    \begin{itemize}
        \item Airflow DAG triggers daily
        \item Data extracted from Alpha Vantage API
        \item Data quality checked
        \item Data transformed and features created
        \item Data versioned with DVC and pushed to S3
        \item Model trained and logged to MLflow
        \item Model registered in Model Registry (Staging)
    \end{itemize}
    \item \textbf{Phase III}:
    \begin{itemize}
        \item PR \#1: Feature → Dev (linting and testing)
        \item PR \#2: Dev → Test (model training and comparison)
        \item PR \#3: Test → Main (model deployment via Docker)
    \end{itemize}
    \item \textbf{Phase IV}:
    \begin{itemize}
        \item FastAPI serves predictions
        \item Prometheus scrapes metrics every 15 seconds
        \item Grafana visualizes metrics in dashboards
        \item Alerts fire when thresholds exceeded
    \end{itemize}
\end{enumerate}

\subsection{Key File Reference}

\begin{longtable}{|p{3cm}|p{4cm}|p{8cm}|}
\hline
\textbf{File} & \textbf{Phase} & \textbf{Purpose} \\
\hline
\texttt{src/config.py} & I & Configuration management \\
\hline
\texttt{src/data\_extraction.py} & II & Fetch data from Alpha Vantage API \\
\hline
\texttt{src/data\_quality\_check.py} & II & Validate data quality \\
\hline
\texttt{src/data\_transformation.py} & II & Create ML features \\
\hline
\texttt{src/train.py} & II & Train model and log to MLflow \\
\hline
\texttt{airflow/dags/stock\_prediction\_dag.py} & II & Orchestrate data pipeline \\
\hline
\texttt{.dvc/config} & II & DVC remote configuration (S3, DAGsHub) \\
\hline
\texttt{.github/workflows/ci-feature-to-dev.yml} & III & Lint and test on PR \#1 \\
\hline
\texttt{.github/workflows/ci-dev-to-test.yml} & III & Train and compare models on PR \#2 \\
\hline
\texttt{.github/workflows/cd-test-to-main.yml} & III & Deploy model via Docker on PR \#3 \\
\hline
\texttt{docker/api/Dockerfile} & III & Containerize FastAPI application \\
\hline
\texttt{src/api.py} & IV & FastAPI app with Prometheus metrics \\
\hline
\texttt{monitoring/prometheus.yml} & IV & Prometheus scrape configuration \\
\hline
\texttt{monitoring/grafana/provisioning/datasources/prometheus.yml} & IV & Grafana datasource configuration \\
\hline
\texttt{monitoring/grafana/provisioning/dashboards/mlops-dashboard.json} & IV & Grafana dashboard definition \\
\hline
\texttt{docker-compose.api.yml} & IV & Orchestrate monitoring services \\
\hline
\texttt{test\_data\_drift.ps1} & IV & Test data drift detection \\
\hline
\end{longtable}

\section{Conclusion}

This document provides a comprehensive guide to the MLOps Stock Prediction Pipeline, explaining how all phases connect, which files handle each process, and detailed explanations of all tools and technologies. The pipeline demonstrates a complete MLOps workflow from data extraction to production deployment with monitoring and observability.

\subsection{Key Takeaways}

\begin{itemize}
    \item \textbf{Phase I} establishes project foundation
    \item \textbf{Phase II} implements automated data pipeline with versioning
    \item \textbf{Phase III} automates testing, comparison, and deployment
    \item \textbf{Phase IV} provides production monitoring and alerting
    \item All phases are interconnected and work together to create a robust MLOps system
\end{itemize}

\vfill
\begin{center}
\textcolor{primaryblue}{\rule{0.8\textwidth}{1pt}}\\[0.5cm]
\textcolor{darkgray}{\textit{End of Document}}\\[0.5cm]
\textcolor{primaryblue}{\rule{0.8\textwidth}{1pt}}
\end{center}

\end{document}

