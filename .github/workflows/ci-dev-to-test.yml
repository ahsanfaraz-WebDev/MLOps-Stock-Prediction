name: CI - Dev to Test (Model Training & Comparison)

on:
  pull_request:
    branches:
      - test
    paths:
      - 'src/**'
      - 'airflow/**'
      - 'requirements.txt'
      - '.github/workflows/**'

jobs:
  train-and-compare:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install dvc[s3] boto3
    
    - name: Configure Git
      run: |
        git config user.name "MLOps CI"
        git config user.email "ci@mlops.local"
    
    - name: Set up DVC
      run: |
        dvc remote list || echo "DVC remotes not configured"
    
    - name: Pull latest data from DVC
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: us-east-1
      run: |
        if [ -n "$AWS_ACCESS_KEY_ID" ] && [ -n "$AWS_SECRET_ACCESS_KEY" ]; then
          echo "AWS credentials configured, attempting DVC pull..."
          dvc pull || echo "DVC pull failed, will try to use existing data or fetch new data"
        else
          echo "AWS credentials not configured, skipping DVC pull"
        fi
      continue-on-error: true
    
    - name: Train model
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
        DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        # Find the latest processed data file
        LATEST_DATA=$(ls -t data/processed/stock_data_processed_*.csv 2>/dev/null | head -1)
        
        if [ -z "$LATEST_DATA" ]; then
          echo "No processed data found. Attempting to run data pipeline..."
          
          # Try to fetch new data (may fail due to API rate limits)
          if python src/data_extraction.py 2>&1 | grep -q "Rate Limit"; then
            echo "⚠️ Alpha Vantage API rate limit reached. Checking for existing raw data..."
            # Check if there's any raw data we can use
            RAW_DATA=$(ls -t data/raw/stock_data_*.csv 2>/dev/null | head -1)
            if [ -n "$RAW_DATA" ]; then
              echo "Found existing raw data: $RAW_DATA"
              python src/data_transformation.py "$RAW_DATA"
              LATEST_DATA=$(ls -t data/processed/stock_data_processed_*.csv 2>/dev/null | head -1)
            else
              echo "❌ No data available: API rate limited and no existing data found"
              echo "Skipping model training - please ensure data is available via DVC or wait for API limit reset"
              exit 1
            fi
          else
            # Data extraction succeeded, now transform
            python src/data_transformation.py data/raw/stock_data_AAPL_*.csv
            LATEST_DATA=$(ls -t data/processed/stock_data_processed_*.csv | head -1)
          fi
        fi
        
        if [ -z "$LATEST_DATA" ]; then
          echo "❌ Error: No processed data file found after attempting data pipeline"
          exit 1
        fi
        
        echo "Training model with data: $LATEST_DATA"
        python src/train.py "$LATEST_DATA"
    
    - name: Set up Node.js for CML
      uses: actions/setup-node@v3
      with:
        node-version: '18'
    
    - name: Install CML
      run: |
        npm install -g @dvcorg/cml
    
    - name: Compare models with CML
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
        DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        # Install MLflow client for model comparison
        pip install mlflow
        
        # Get current model metrics from MLflow
        python << EOF
        import mlflow
        import os
        import sys
        
        mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))
        if os.environ.get('DAGSHUB_USERNAME') and os.environ.get('DAGSHUB_TOKEN'):
            os.environ['MLFLOW_TRACKING_USERNAME'] = os.environ.get('DAGSHUB_USERNAME')
            os.environ['MLFLOW_TRACKING_PASSWORD'] = os.environ.get('DAGSHUB_TOKEN')
        
        client = mlflow.tracking.MlflowClient()
        
        # Get the latest run (current model)
        from src.config import MODEL_NAME
        model_worse = False
        
        try:
            latest_run = client.search_runs(experiment_ids=["0"], max_results=1, order_by=["start_time DESC"])[0]
            current_test_rmse = latest_run.data.metrics.get('test_rmse', None)
            current_test_r2 = latest_run.data.metrics.get('test_r2', None)
            
            # Build report
            report_lines = ["## Model Performance Comparison\n\n"]
            
            # Try to get production model metrics
            try:
                prod_versions = client.get_latest_versions(MODEL_NAME, stages=["Production"])
                if prod_versions:
                    prod_version = prod_versions[0]
                    prod_run = client.get_run(prod_version.run_id)
                    prod_test_rmse = prod_run.data.metrics.get('test_rmse', None)
                    prod_test_r2 = prod_run.data.metrics.get('test_r2', None)
                    
                    report_lines.append("### Current Model (from this PR)\n")
                    report_lines.append(f"- Test RMSE: {current_test_rmse:.4f}\n")
                    report_lines.append(f"- Test R²: {current_test_r2:.4f}\n\n")
                    report_lines.append("### Production Model\n")
                    report_lines.append(f"- Test RMSE: {prod_test_rmse:.4f}\n")
                    report_lines.append(f"- Test R²: {prod_test_r2:.4f}\n\n")
                    
                    # Determine if model is better
                    if current_test_rmse and prod_test_rmse:
                        rmse_improvement = ((prod_test_rmse - current_test_rmse) / prod_test_rmse) * 100
                        if current_test_rmse < prod_test_rmse:
                            report_lines.append(f"✅ **Model Improvement**: RMSE improved by {rmse_improvement:.2f}%\n")
                        else:
                            report_lines.append(f"❌ **Model Degradation**: RMSE worsened by {abs(rmse_improvement):.2f}%\n")
                            report_lines.append("\n⚠️ **Merge should be blocked** - New model performs worse than production.\n")
                            model_worse = True
                else:
                    # No production model, just report current metrics
                    report_lines.append("### Current Model\n")
                    report_lines.append(f"- Test RMSE: {current_test_rmse:.4f}\n")
                    report_lines.append(f"- Test R²: {current_test_r2:.4f}\n\n")
                    report_lines.append("ℹ️ No production model found for comparison.\n")
            except Exception as e:
                # No production model in registry yet
                report_lines.append("### Current Model\n")
                report_lines.append(f"- Test RMSE: {current_test_rmse:.4f}\n")
                report_lines.append(f"- Test R²: {current_test_r2:.4f}\n\n")
                report_lines.append("ℹ️ No production model found for comparison.\n")
        except Exception as e:
            print(f"Error comparing models: {e}")
            report_lines = ["## Model Training Results\n\n"]
            report_lines.append(f"⚠️ Error retrieving model metrics: {str(e)}\n")
        
        # Write report to file for CML
        report_body = "".join(report_lines)
        with open('report.md', 'w') as f:
            f.write(report_body)
        
        print("\n" + "="*60)
        print("MODEL COMPARISON REPORT")
        print("="*60)
        print(report_body)
        print("="*60 + "\n")
        
        # Exit with error if model is worse (before CML posts comment)
        if model_worse:
            sys.exit(1)
        EOF
        
        # Post CML report as PR comment
        if [ -f "report.md" ]; then
          cml comment create report.md || echo "CML comment failed, continuing..."
        fi
    
    - name: Check model performance
      run: |
        # Verify model file exists
        if [ ! -f "models/stock_model.pkl" ]; then
          echo "Model training failed - no model file generated"
          exit 1
        fi
        echo "Model training completed successfully"


