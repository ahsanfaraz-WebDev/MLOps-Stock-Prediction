name: CI - Dev to Test (Model Training & Comparison)

on:
  pull_request:
    branches:
      - test
    paths:
      - 'src/**'
      - 'airflow/**'
      - 'requirements.txt'
      - '.github/workflows/**'

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  train-and-compare:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install dvc[s3] boto3
    
    - name: Configure Git
      run: |
        git config user.name "MLOps CI"
        git config user.email "ci@mlops.local"
    
    - name: Set up DVC
      run: |
        dvc remote list || echo "DVC remotes not configured"
    
    - name: Configure AWS credentials
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: us-east-1
      run: |
        echo "AWS credentials configured"
    
    - name: Pull latest data from DVC
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: us-east-1
      run: |
        if [ -n "$AWS_ACCESS_KEY_ID" ] && [ -n "$AWS_SECRET_ACCESS_KEY" ]; then
          echo "AWS credentials configured, attempting DVC pull..."
          dvc pull || echo "DVC pull failed, continuing..."
        else
          echo "AWS credentials not configured, skipping DVC pull"
        fi
      continue-on-error: true
    
    - name: Train model
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
        DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        ALPHA_VANTAGE_KEY: ${{ secrets.ALPHA_VANTAGE_KEY }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        # Find the latest processed data file
        LATEST_DATA=$(ls -t data/processed/stock_data_processed_*.csv 2>/dev/null | head -1)
        if [ -z "$LATEST_DATA" ]; then
          echo "No processed data found. Running data pipeline..."
          # Run data extraction and transformation
          python src/data_extraction.py
          python src/data_transformation.py data/raw/stock_data_AAPL_*.csv
          LATEST_DATA=$(ls -t data/processed/stock_data_processed_*.csv | head -1)
        fi
        echo "Training model with data: $LATEST_DATA"
        python src/train.py "$LATEST_DATA"
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
    
    - name: Install CML
      run: |
        npm install -g @dvcorg/cml
    
    - name: Compare models and post report
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
        DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        # Install MLflow client for model comparison
        pip install mlflow python-dotenv
        
        # Get current model metrics from MLflow and generate report
        python << EOF
        import mlflow
        import os
        import sys
        
        mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))
        if os.environ.get('DAGSHUB_USERNAME') and os.environ.get('DAGSHUB_TOKEN'):
            os.environ['MLFLOW_TRACKING_USERNAME'] = os.environ.get('DAGSHUB_USERNAME')
            os.environ['MLFLOW_TRACKING_PASSWORD'] = os.environ.get('DAGSHUB_TOKEN')
        
        client = mlflow.tracking.MlflowClient()
        
        # Get the latest run (current model)
        from src.config import MODEL_NAME
        model_worse = False
        
        try:
            latest_run = client.search_runs(experiment_ids=["0"], max_results=1, order_by=["start_time DESC"])[0]
            current_test_rmse = latest_run.data.metrics.get('test_rmse', None)
            current_test_r2 = latest_run.data.metrics.get('test_r2', None)
            
            # Build report
            report_lines = ["## Model Performance Comparison\n\n"]
            
            # Try to get production model metrics
            try:
                prod_versions = client.get_latest_versions(MODEL_NAME, stages=["Production"])
                if prod_versions:
                    prod_version = prod_versions[0]
                    prod_run = client.get_run(prod_version.run_id)
                    prod_test_rmse = prod_run.data.metrics.get('test_rmse', None)
                    prod_test_r2 = prod_run.data.metrics.get('test_r2', None)
                    
                    report_lines.append("### Current Model (from this PR)\n")
                    report_lines.append(f"- Test RMSE: {current_test_rmse:.4f}\n")
                    report_lines.append(f"- Test RÂ²: {current_test_r2:.4f}\n\n")
                    report_lines.append("### Production Model\n")
                    report_lines.append(f"- Test RMSE: {prod_test_rmse:.4f}\n")
                    report_lines.append(f"- Test RÂ²: {prod_test_r2:.4f}\n\n")
                    
                    # Determine if model is better
                    if current_test_rmse and prod_test_rmse:
                        rmse_improvement = ((prod_test_rmse - current_test_rmse) / prod_test_rmse) * 100
                        if current_test_rmse < prod_test_rmse:
                            report_lines.append(f"âœ… **Model Improvement**: RMSE improved by {rmse_improvement:.2f}%\n")
                            report_lines.append("\nâœ… **Merge Approved** - New model performs better than production.\n")
                        else:
                            report_lines.append(f"âŒ **Model Degradation**: RMSE worsened by {abs(rmse_improvement):.2f}%\n")
                            report_lines.append("\nðŸš« **Merge Blocked** - New model performs worse than production.\n")
                            report_lines.append("\n**Action Required**: Please improve the model before merging.\n")
                            model_worse = True
                else:
                    # No production model, just report current metrics
                    report_lines.append("### Current Model\n")
                    report_lines.append(f"- Test RMSE: {current_test_rmse:.4f}\n")
                    report_lines.append(f"- Test RÂ²: {current_test_r2:.4f}\n\n")
                    report_lines.append("â„¹ï¸ No production model found for comparison.\n")
                    report_lines.append("âœ… **Merge Approved** - This will be the first production model.\n")
            except Exception as e:
                # No production model in registry yet
                report_lines.append("### Current Model\n")
                report_lines.append(f"- Test RMSE: {current_test_rmse:.4f}\n")
                report_lines.append(f"- Test RÂ²: {current_test_r2:.4f}\n\n")
                report_lines.append("â„¹ï¸ No production model found for comparison.\n")
                report_lines.append("âœ… **Merge Approved** - This will be the first production model.\n")
        except Exception as e:
            print(f"Error comparing models: {e}")
            report_lines = ["## Model Training Results\n\n"]
            report_lines.append(f"âš ï¸ Error retrieving model metrics: {str(e)}\n")
            report_lines.append("\nðŸš« **Merge Blocked** - Unable to verify model performance.\n")
            model_worse = True
        
        # Write report to file for CML
        report_body = "".join(report_lines)
        with open('report.md', 'w') as f:
            f.write(report_body)
        
        # Write exit code to file so bash can read it
        with open('model_check_result.txt', 'w') as f:
            f.write('1' if model_worse else '0')
        
        print("\n" + "="*60)
        print("MODEL COMPARISON REPORT")
        print("="*60)
        print(report_body)
        print("="*60 + "\n")
        
        if model_worse:
            print("âŒ Model performance check FAILED - Merge will be blocked")
        else:
            print("âœ… Model performance check PASSED - Merge approved")
        EOF
        
        # Read the exit code from file
        MODEL_CHECK_FAILED=$(cat model_check_result.txt)
        
        # Always post CML report (even if model is worse)
        if [ -f "report.md" ]; then
          echo "Posting CML report to PR..."
          cml comment create report.md || echo "CML comment failed, but continuing..."
        fi
        
        # Exit with error to block merge if model is worse
        if [ "$MODEL_CHECK_FAILED" = "1" ]; then
          echo "âŒ Model performance check failed - Blocking merge"
          exit 1
        fi
        
        echo "âœ… Model performance check passed"
    
    - name: Check model performance
      run: |
        # Verify model file exists
        if [ ! -f "models/stock_model.pkl" ]; then
          echo "Model training failed - no model file generated"
          exit 1
        fi
        echo "Model training completed successfully"


